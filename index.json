[{"categories":null,"contents":"Addressed pretty significant page load performance issue founde in larger deployments. Eliminates uses of intensive backend query, replacing it with an asynchronous API call against a lucene index. This change reduces page load from from 2+ minutes to nearly instant, with an incredibly responsive UI.\n","permalink":"https://h-neco.github.io/projects/contributions/deploy-triggers/","tags":["Java","jQuery","REST APIs","Bamboo","JSON"],"title":"Atlassian Deployment Triggers"},{"categories":null,"contents":"This talk looked at Liberty Mutual’s transformation to Continuous Integration, Continuous Delivery, and DevOps. For a large, heavily regulated industry, this task can not only be daunting, but viewed by many as impossible. Often, organizations try to reduce the friction through micro-fixes, but Eddie’s team asked how to change the culture to reduce the friction and concluded with the following final points:\nDon’t mandate DevOps. Give employees the chance to master their discipline with examples to set and follow. Favor deep end-to-end accomplishments over broad but incremental steps forward. Focus on taking the right teams far before encouraging broad adoption. Centralize the platforms and tools that your teams shouldn’t be thinking about. Provide foundational services/commodities and let teams stay on purpose. Incorporate contributions from everyone; don’t stifle autonomy. Stay open to new ways of working. Challenge security policies, but respect intentions. Find new ways to enforce concerns without abandoning precaution. ","permalink":"https://h-neco.github.io/publications/alldaydevops/","tags":["DevOps","Continuous Integration","Continuous Delivery","CI/CD pipelines","agile","Culture"],"title":"Organically DevOps: Building Quality and Security into the Software Supply Chain at Liberty Mutual"},{"categories":null,"contents":"Shields.io is a massive library of badges that can be inserted into project README\u0026rsquo;s or websites displaying various statuses (code coverage, health, version, etc). Support for docker was missing the current build health, and was a pretty trivial addition.\n","permalink":"https://h-neco.github.io/projects/contributions/shields-docker/","tags":["Docker","Rest APIs","JavaScript","node.js","JSON"],"title":"Added Docker Build Status Badge to shields.io"},{"categories":null,"contents":"While adding Structured Data to a client\u0026rsquo;s website I found some example JSON that was invalid. Simple contribution to cleanup the user documentation providing syntactically valid JSON documents.\n","permalink":"https://h-neco.github.io/projects/contributions/schema-org/","tags":["JSON"],"title":"Schema.org Structured Data documentation fixes"},{"categories":null,"contents":"Intro EFSバーストモードのクレジット枯渇対策メモ EFSをバーストモードで使用していて、クレジットが不足してきたため、私が取った対策をメモとして提供します。 技術要素 EFS aws EFSの2つのモード EFSには2つのモードがあります: バーストモード（汎用）とプロビジョンドモード。\nバーストモードは、使用されているストレージの量に基づいてトラフィックを自動的に調整し、一時的なトラフィックの増加に対応できます。バースト可能な帯域幅はトラフィックの使用状況によって異なり、最低105 Mbpsのバーストが可能です。ただし、1秒あたりの読み取り/書き込みリクエストの数には制限があり、制限を超えるとスループットが低下する可能性があります。\nプロビジョンドモードでは、ボリュームのスループット、最小/最大/バースト、および書き込みスループットを設定できます。読み取りスループットは書き込みスループットの3倍です。プロビジョンドモードはトラフィックに基づいた自動スケーリングはありませんが、スループットの設定に基づいて必要なトラフィックに対応できます。\nバーストモード（汎用）の注意事項と対策 バーストモード（汎用）の注意事項と対策\n注意事項1: バーストクレジット\nバーストモードは、ファイルの読み取り/書き込み操作から蓄積されたクレジットを消費し、NAS上のデータ使用状況に基づいてクレジットを補充します。バーストモードの最低速度は105 Mbpsです。ただし、バーストクレジットが枯渇すると、スループットの性能が著しく低下し、マウントシステムからのファイル参照がタイムアウトする場合があります。\n確認方法:\nメトリック名: BurstCreditBalance 最初に2.3Tのクレジットが提供されます。 バーストクレジットの枯渇を避けるためには、以下の対策を取ることができます:\nクレジットの回復を加速するために大量のデータを配置する。 プロビジョンドモードに切り替えて一定のスループットを確保する。 注意事項2: リクエスト制限\nバーストモードにはリクエストの制限があります。 リード用の最大IOPSは35,000、ライト用の最大IOPSは7,000です。 バーストモードの最大IOPSを超えると、スループットのパフォーマンスが低下します。したがって、高いIOPSが必要な場合は、プロビジョンドモードの使用を検討してください。 確認方法: メトリック名: PercentIOLimit 検討に基づく対策の取得 バーストモードに大量のデータを配置してクレジットの回復速度を加速するという戦略を採用しました。\n10GBのファイルを配置する試み。 コスト: $3/月 コマンド: $ dd if=/dev/zero of=10GB_file_1 bs=10240k count=1000 確認方法: メトリック名: BurstCreditBalance 高いコストのため、プロビジョンドモードは使用していません。 スループット 33(Mib/s) / 最大読み取りスループット 99(Mib/s) コスト: $238/月 スループット 15(Mib/s) / 最大読み取りスループット 45(Mib/s) コスト: $108/月 スループット 5(Mib/s) / 最大読み取りスループット 15(Mib/s) コスト: $36/月 ","permalink":"https://h-neco.github.io/blog/aws-efs/","tags":["efs","aws"],"title":"EFS Burst Mode のクレジット枯渇対策メモ"},{"categories":null,"contents":"Intro デフォルトでは、EC2インスタンスが終了すると、アタッチされたEBSボリュームも削除されます。しかし、AWS CLIを使用してこれらを永続化する方法を探ってみましょう。 技術要素 EC2/EBS コマンド タグによってフィルタリングされた対象インスタンスのボリューム情報を取得します。 DeleteOnTerminationがtrueである場合、ボリュームは永続化されていないことを意味します。 $ aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=xxxxx-prod-web01\u0026#34; | jq -r .Reservations[0].Instances[0].BlockDeviceMappings [ { \u0026#34;DeviceName\u0026#34;: \u0026#34;/dev/sda1\u0026#34;, \u0026#34;Ebs\u0026#34;: { \u0026#34;AttachTime\u0026#34;: \u0026#34;2023-04-18T04:59:14+00:00\u0026#34;, \u0026#34;DeleteOnTermination\u0026#34;: true, \u0026#34;Status\u0026#34;: \u0026#34;attached\u0026#34;, \u0026#34;VolumeId\u0026#34;: \u0026#34;vol-xxxxxxxxxxxx\u0026#34; } } ] 設定ファイルを準備します。 DeleteOnTerminationをfalseに設定します。 $ vim mapping.json [ { \u0026#34;DeviceName\u0026#34;: \u0026#34;/dev/sda1\u0026#34;, \u0026#34;Ebs\u0026#34;: { \u0026#34;DeleteOnTermination\u0026#34;: false } } ] インスタンスの設定変更 $ aws ec2 modify-instance-attribute --instance-id \u0026#34;i-xxxxxxxxxxxxxx\u0026#34; --block-device-mappings file://mapping.json タグでフィルタリングされた対象インスタンスのボリューム情報を取得します。 DeleteOnTerminationがfalseであることを確認します。 $ aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=xxxxx-prod-web01\u0026#34; | jq -r .Reservations[0].Instances[0].BlockDeviceMappings [ { \u0026#34;DeviceName\u0026#34;: \u0026#34;/dev/sda1\u0026#34;, \u0026#34;Ebs\u0026#34;: { \u0026#34;AttachTime\u0026#34;: \u0026#34;2023-04-18T04:59:14+00:00\u0026#34;, \u0026#34;DeleteOnTermination\u0026#34;: false, \u0026#34;Status\u0026#34;: \u0026#34;attached\u0026#34;, \u0026#34;VolumeId\u0026#34;: \u0026#34;vol-xxxxxxxxxxxx\u0026#34; } } ] ","permalink":"https://h-neco.github.io/blog/aws-ec2-ebs-delete-on-termination/","tags":["ec2","aws-cli","aws"],"title":"Persisting an Attached EBS Volume to EC2 Using AWS CLI."},{"categories":null,"contents":"Intro 以前は、GitHubやBitbucketから個人のAWSへのデプロイ時にアクセスキーとシークレットキーを使用していました。しかし、これを管理するのが煩雑になったため、OIDCに切り替えました。\nOIDC (OpenID Connect) は、認証と認可のためのオープンスタンダードです。OIDCを使用することで、AWSへのアクセスを安全に管理し、認証プロバイダを介してシームレスなアクセス体験を提供できます。OIDCを使用する場合、AWSへのアクセスにはアクセストークンが使用されます。\nOIDCを導入することで、アクセスキーやシークレットキーを個別に管理する必要がなくなり、セキュリティと利便性が向上します。\nやること TerraformでOIDCを設定する方法 GitHub Bitbucket OIDCを使用してデプロイする方法 シンプルなGitHub ActionsとBitbucket Pipelinesの作成 技術要素 OIDC AWS Bitbucket GitHub Terraform What is OIDC? OIDC（OpenID Connect）は、OAuth 2.0プロトコルを拡張し、Webアプリケーションやモバイルアプリケーションにおけるユーザー認証の仕組みを提供する認証プロトコルです。\nBenefits of OIDC GitからのデプロイにOIDCを使用することで、いくつかの利点があります。それには、Gitを介したデプロイのセキュリティの向上、トークンの有効期限の管理の容易化、MFAが有効化されていないアクセスキーの使用に関連するセキュリティリスクの回避などがあります。ただし、MFAが有効化されたアクセスキーを使用する場合は、デバイス管理などの要素を考慮する必要があるため、複雑な場合があります。OIDCを使用することで、ユーザーはIDプロバイダーの認証資格情報を使用できるため、認証情報の管理が簡素化され、セキュリティが向上します。\n構築背景 以前は、AWSへのデプロイは手間がかかるものでした。CLIの代わりに管理コンソールからデプロイすることを考えるほどです。\n以前のフローでは、キーの取得に不便さが伴いました： アプリからMFAトークンを確認してコマンドを実行する： $ aws sts get-session-token \u0026ndash;serial-number arn:aws:iam::xxxxxx:mfa/xxxxxx \u0026ndash;token-code xxxxxx 取得したキーをエクスポートするか、Gitに登録する セッションの有効期限が切れるたびに定期的にコマンドを実行する\u0026hellip; TerraformでOIDCを構築する Terraformでの構築 GitHub Actionsを使用するシナリオとBitbucket Pipelinesを使用するシナリオの2つを説明します。 GitHub Sample variable \u0026#34;aws_account_id\u0026#34; {} variable \u0026#34;github_repo_name\u0026#34; {} variable \u0026#34;oidc_token_url\u0026#34; { default = \u0026#34;https://token.actions.githubusercontent.com\u0026#34; } data \u0026#34;tls_certificate\u0026#34; \u0026#34;github_oidc_token\u0026#34; { url = var.oidc_token_url } resource \u0026#34;aws_iam_openid_connect_provider\u0026#34; \u0026#34;github_oidc_provider\u0026#34; { url = var.oidc_token_url client_id_list = [ \u0026#34;sts.amazonaws.com\u0026#34; ] thumbprint_list = [data.tls_certificate.github_oidc_token.certificates.0.sha1_fingerprint] } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;github_oidc_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Federated\u0026#34; identifiers = [\u0026#34;arn:aws:iam::${var.aws_account_id}:oidc-provider/oidc-provider/token.actions.githubusercontent.com\u0026#34;] } condition { test = \u0026#34;StringEquals\u0026#34; variable = \u0026#34;token.actions.githubusercontent.com:sub\u0026#34; values = [\u0026#34;repo:${var.github_repo_name}:ref:refs/heads/main\u0026#34;] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;github_oidc_role\u0026#34; { name = \u0026#34;GithubOIDC-TEST\u0026#34; description = \u0026#34;GithubOIDC-TEST\u0026#34; assume_role_policy = data.aws_iam_policy_document.github_oidc_policy.json } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;github_oidc_administrator_access_attachment\u0026#34; { role = aws_iam_role.github_oidc_role.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/AdministratorAccess\u0026#34; } Bitbucket Sample variable \u0026#34;bitbucket_oidc_url\u0026#34; {} variable \u0026#34;bitbucket_oidc_audience\u0026#34; {} variable \u0026#34;account_id\u0026#34; {} variable \u0026#34;git_space\u0026#34; {} data \u0026#34;tls_certificate\u0026#34; \u0026#34;bitbucket\u0026#34; { url = var.bitbucket_oidc_url } resource \u0026#34;aws_iam_openid_connect_provider\u0026#34; \u0026#34;bitbucket\u0026#34; { url = var.bitbucket_oidc_url client_id_list = [ var.bitbucket_oidc_audience, ] thumbprint_list = [data.tls_certificate.bitbucket.certificates.0.sha1_fingerprint] } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;bitbucket_oidc_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Federated\u0026#34; identifiers = [\u0026#34;arn:aws:iam::${var.account_id}:oidc-provider/api.bitbucket.org/2.0/workspaces/${var.git_space}/pipelines-config/identity/oidc\u0026#34;] } condition { test = \u0026#34;StringEquals\u0026#34; variable = \u0026#34;api.bitbucket.org/2.0/workspaces/${var.git_space}/pipelines-config/identity/oidc:aud\u0026#34; values = [var.bitbucket_oidc_audience] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;bitbucket_oidc_role\u0026#34; { name = \u0026#34;BitbucketOIDC-TEST\u0026#34; description = \u0026#34;BitbucketOIDC-TEST\u0026#34; assume_role_policy = data.aws_iam_policy_document.bitbucket_oidc_policy.json } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;administrator_access_attachment\u0026#34; { role = aws_iam_role.bitbucket_oidc_role.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/AdministratorAccess\u0026#34; } Testing OIDC Setup Github Action The sample code deploys statically built Hugo content to S3 using OIDC. name: s3-deploy on: push: branches: - main jobs: s3put: runs-on: ubuntu-latest permissions: id-token: write contents: read steps: - name: Checkout code uses: actions/checkout@v2 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.111.3\u0026#39; - name: Build run: hugo --minify - uses: aws-actions/configure-aws-credentials@v1 with: aws-region: \u0026#39;ap-northeast-1\u0026#39; # Specify the region role-to-assume: \u0026#39;arn:aws:iam::xxxxxxxxxxx:role/oidc-role\u0026#39; # ARN of the created IAM role - name: Deploy run: aws s3 sync --delete public s3://my-s3-bucket/ Bitbucket Pipelines The sample code syncs the \u0026ldquo;public\u0026rdquo; folder from the master branch to my-s3-bucket using OIDC. image: amazon/aws-cli pipelines: default: - step: \u0026amp;s3-deploy name: Deploy to S3 with OIDC oidc: true script: - export AWS_WEB_IDENTITY_TOKEN_FILE=$(pwd)/web-identity-token - export AWS_ROLE_ARN=\u0026#39;created role\u0026#39; - echo $BITBUCKET_STEP_OIDC_TOKEN \u0026gt; $(pwd)/web-identity-token - aws s3 sync --delete public s3://my-s3-bucket/ branches: master: - step: *s3-deploy Setting up OIDC with CloudFormation Parameters: AwsAccountId: Type: String Description: AWS Account ID GithubRepoName: Type: String Description: Name of the GitHub repository OidcTokenUrl: Type: String Default: https://token.actions.githubusercontent.com Resources: GithubOidcTokenCertificate: Type: AWS::CloudFormation::CustomResource Version: \u0026#34;1.0\u0026#34; Properties: ServiceToken: !Sub arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:token-actions-github Url: !Ref OidcTokenUrl GithubOidcProvider: Type: AWS::IAM::OpenIDConnectProvider Properties: Url: !Ref OidcTokenUrl ClientIDList: - sts.amazonaws.com ThumbprintList: - !GetAtt GithubOidcTokenCertificate.Sha1Fingerprint GithubOidcPolicyDocument: Type: AWS::IAM::Policy Properties: PolicyName: GithubOIDCPolicy PolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: Allow Action: sts:AssumeRoleWithWebIdentity Resource: \u0026#34;*\u0026#34; Condition: StringEquals: token.actions.githubusercontent.com:sub: - !Sub \u0026#34;repo:${GithubRepoName}:ref:refs/heads/main\u0026#34; Principal: Federated: !Sub arn:aws:iam::${AwsAccountId}:oidc-provider/oidc-provider/token.actions.githubusercontent.com GithubOidcRole: Type: AWS::IAM::Role Properties: RoleName: GithubOIDC-TEST AssumeRolePolicyDocument: !Ref GithubOidcPolicyDocument GithubOidcAdministratorAccessAttachment: Type: AWS::IAM::PolicyAttachment Properties: PolicyArn: arn:aws:iam::aws:policy/AdministratorAccess Roles: - !Ref GithubOidcRole ","permalink":"https://h-neco.github.io/blog/cicd-oidc/","tags":["OIDC","CI/CD","github-action","bitbucket-pipelines","terraform"],"title":"OIDCでのデプロイメモ (Git to AWS)"},{"categories":null,"contents":"Intro PCの入れ替えに伴い、Lambdaのローカル実行環境を再構築した。メモとして残しておきます。 PCリプレースのついでにLambdaのローカル実行環境を再構築。メモとして残しておく。\n技術要素 Volta Lambda LocalStack TypeScript SAM 前提条件 Installing Volta Voltaのインストール Volta は Node.js のバージョン管理に特化したツールです。プロジェクトごとに異なるNode.jsのバージョンを切り替えることができ、Node.jsのバージョンを手動でインストール・管理することなく、異なるバージョンを簡単に管理することができます。Voltaは、他のバージョン管理ツールと比較して切り替えが簡単という利点があり、OSやシェルに依存しないため様々な環境で利用することができます。\n$ curl https://get.volta.sh | bash $ echo \u0026#39;export VOLTA_HOME=\u0026#34;$HOME/.volta\u0026#34;\u0026#39; \u0026gt;\u0026gt; .zshrc $ echo \u0026#39;export PATH=\u0026#34;$VOLTA_HOME/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; .zshrc Node.jsとYarnのインストール 利用可能なバージョンを確認するには、volta list nodeを使用します。特定のバージョンをインストールしたい場合は、volta install node@18.15.0 というフォーマットを使ってください。\n$ volta install node success: installed and set node@18.15.0 (with npm@9.5.0) as default $ volta install yarn success: installed and set yarn@4.0.0-rc.42 as default SAM のインストール 以下の手順はmacOS用である。Windowsユーザーの方は公式サイトからMSIファイルをダウンロードすることをお勧めします。\nmacOS $ brew tap aws/tap $ brew install aws-sam-cli $ sam --version SAM CLI, version 1.78.0 Creating a Project $ sam init -r nodejs18.x SAM CLI now collects telemetry to better understand customer needs. You can OPT OUT and disable telemetry collection by setting the environment variable SAM_CLI_TELEMETRY=0 in your shell. Thanks for your help! Learn More: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-telemetry.html Which template source would you like to use? 1 - AWS Quick Start Templates 2 - Custom Template Location Choice: 1 Choose an AWS Quick Start application template 1 - Hello World Example 2 - Hello World Example With Powertools 3 - Multi-step workflow 4 - Standalone function 5 - Scheduled task 6 - Data processing 7 - Serverless API Template: 1 Based on your selections, the only Package type available is Zip. We will proceed to selecting the Package type as Zip. Based on your selections, the only dependency manager available is npm. We will proceed copying the template using npm. Select your starter template 1 - Hello World Example 2 - Hello World Example TypeScript Template: 2 Would you like to enable X-Ray tracing on the function(s) in your application? [y/N]: n Would you like to enable monitoring using CloudWatch Application Insights? For more info, please view https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.html [y/N]: n Project name [sam-app]: sam-local-study Cloning from https://github.com/aws/aws-sam-cli-app-templates (process may take a moment) ----------------------- Generating application: ----------------------- Name: sam-local-study Runtime: nodejs18.x Architectures: x86_64 Dependency Manager: npm Application Template: hello-world-typescript Output Directory: . Configuration file: sam-local-study/samconfig.toml Next steps can be found in the README file at sam-local-study/README.md Commands you can use next ========================= [*] Create pipeline: cd sam-local-study \u0026amp;\u0026amp; sam pipeline init --bootstrap [*] Validate SAM template: cd sam-local-study \u0026amp;\u0026amp; sam validate [*] Test Function in the Cloud: cd sam-local-study \u0026amp;\u0026amp; sam sync --stack-name {stack-name} --watch プロンプトに従うと、以下のファイルが生成される：\n$ tree . . └── sam-local-study ├── README.md ├── events │ └── event.json ├── hello-world │ ├── app.ts │ ├── jest.config.ts │ ├── package.json │ ├── tests │ │ └── unit │ │ └── test-handler.test.ts │ └── tsconfig.json ├── samconfig.toml └── template.yaml 6 directories, 9 files メッセージを返すサンプル Lambda 関数の作成 メッセージを返す Lambda 関数のサンプルを作成し、SAM を使って実行してみましょう。\nコードの準備 $ tree . . └── sam-local-study ├── src │ ├── handlers │ └── hello-from-lambda.js └── template.yaml src/handlers/hello-from-lambda.js exports.helloFromLambdaHandler = async () =\u0026gt; { const message = \u0026#39;Hello from Lambda!\u0026#39;; console.info(`${message}`); return message; } template.yaml Resources: HelloWorldFunction: Type: AWS::Serverless::Function Properties: Handler: src/handlers/hello-from-lambda.helloFromLambdaHandler Runtime: nodejs18.x MemorySize: 128 Timeout: 100 Description: A Lambda function that returns a static string. Policies: - AWSLambdaBasicExecutionRole \u0026ldquo;Hello \u0026ldquo;を返すラムダ関数の実行 Execute the command: $ sam local invoke HelloWorldFunction Output: Invoking src/handlers/hello-from-lambda.helloFromLambdaHandler (nodejs18.x) Local image is up-to-date Using local image: public.ecr.aws/lambda/nodejs:18-rapid-x86_64. Mounting /lambda-sam-localstack/sam-local-study as /var/task:ro,delegated, inside runtime container START RequestId: 6d2615de-b38f-4300-bf87-704e1fa6296a Version: $LATEST 2023-04-05T08:41:27.339Z\t6d2615de-b38f-4300-bf87-704e1fa6296a\tINFO\tHello from Lambda! END RequestId: 6d2615de-b38f-4300-bf87-704e1fa6296a REPORT RequestId: 6d2615de-b38f-4300-bf87-704e1fa6296a\tInit Duration: 0.75 ms\tDuration: 953.12 ms\tBilled Duration: 954 ms\tMemory Size: 128 MB\tMax Memory Used: 128 MB \u0026#34;Hello from Lambda!\u0026#34;% LocalStackを使う これについてはまた後日書きます\u0026hellip;。\n","permalink":"https://h-neco.github.io/blog/aws-lambda-local-execution/","tags":["volta","Lambda","LocalStack","TypeScript","SAM"],"title":"Lambdaローカル開発環境構築メモ (SAM | LocalStack | TypeScript)"},{"categories":null,"contents":"Intro TerraformのWrapperツール、Terragrunt導入の検証を行いました。 導入の検証 通常、私はTerraformを使用していますが、次のような不便さがありました： 各デプロイの影響範囲を最小限に抑えるために、Gitリポジトリをより小さなものに分割する必要がありました。Terraformのバージョンアップが面倒でした。 コードからリソースの依存関係を理解するのが難しかったです。 depends_onが明示的に使用できない場所でデプロイ順序に注意する必要がありました。 導入後の課題 導入に関して、以下の2点について検証が保留されています。もし詳しい方がいたら相談に乗っていただけると幸いです。 Terraformの管理だけでなく、Terragruntのバージョン管理も行う必要があります。バージョンアップのプロセスはどのように変わるのか もしTerragruntの開発が停止した場合、スムーズにTerraformのコードに戻せるか 技術要素 terraform terragrunt aws Terragruntとは Terragruntは、Terraformを使用してインフラストラクチャを管理するための作業を簡素化するためのTerraform Wrapperとして知られるオープンソースのツールです。TerragruntはTerraformが提供する機能を拡張し、モジュールのコード再利用、柔軟な構成、再利用性の向上を可能にします。特に、Terraformを使用して複数の環境やアカウントを管理する際にTerragruntは非常に便利です。\nインストール $ brew install tfenv # Installs Terraform (version specified in .terraform-version) $ brew install tgenv # Installs Terragrunt (version specified in .terragrunt-version) ###　フォルダ構成\nこのフォルダ構成は、他の記事を参考に作成されました。この構造の利点は、各環境ごとに異なる変数を指定できるため、コードの再利用性が向上することです。\n$ tree . . ├── README.md ├── docs │ └── graph-dependencies.png ├── envs │ ├── prod │ │ ├── ResourceGroupA │ │ │ └── terragrunt.hcl │ │ ├── ResourceGroupB │ │ │ └── terragrunt.hcl │ │ └── env.hcl │ └── terragrunt.hcl └── modules ├── ResourceGroupA │ └── xx.tf └── ResourceGroupB └── xx.tf 共通ファイル（envs/terragrunt.hcl） このファイルには、バックエンドとしてAWS S3バケットに*.tfstateファイルを保存するための設定が含まれており、各環境に独自の*.tfstateファイルをS3バケット内に保存することができます。また、AWSリソースを作成するためにTerraformが使用するプロバイダの定義も含まれています。設定では、各AWSリージョンごとに異なるプロバイダ設定が指定されています。\n*.tfstateファイルを使用してリソースの状態を管理することで、複数の人がプロジェクトに取り組んでいても競合を回避することができます。\n# Configuration for storing *.tfstate files for each environment remote_state { backend = \u0026#34;s3\u0026#34; config = { bucket = \u0026#34;tfstate-xxxxxxxxxxxxxx\u0026#34; # Stored in `stg/modA.tfstate` key = \u0026#34;${path_relative_to_include()}.tfstate\u0026#34; region = \u0026#34;ap-northeast-1\u0026#34; encrypt = true } generate = { path = \u0026#34;backend.tf\u0026#34; if_exists = \u0026#34;overwrite\u0026#34; } } generate \u0026#34;provider\u0026#34; { path = \u0026#34;provider.tf\u0026#34; if_exists = \u0026#34;overwrite_terragrunt\u0026#34; contents = \u0026lt;\u0026lt;EOF terraform { required_version = \u0026#34;\u0026gt;= 1.3.7\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.53.0\u0026#34; } } } # Tokyo region provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-1\u0026#34; alias = \u0026#34;tokyo\u0026#34; } # Virginia region provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; alias = \u0026#34;virginia\u0026#34; } # ... Other regions EOF } 環境固有のファイル（envs/prod/env.hcl） このファイルは、Terragruntの環境固有のファイルであり、prod環境の設定が含まれています。これらの変数は、他のTerragruntファイルで使用され、環境固有の設定を定義するために使用されます。具体的には、envs/prod/ResourceGroupAのhclファイルで呼び出され、Terraformコードのmodule/ResourceGroupAにパラメータとして渡されます。\nlocals { ENV = \u0026#34;prod\u0026#34; PROJECT_NAME = \u0026#34;test\u0026#34; ACCOUNT_ID = \u0026#34;123456789\u0026#34; VPC_CIDE = \u0026#34;10.0.0.0/24\u0026#34; } 各リソースグループのためのHCLファイル 「ResourceGroupA」と「ResourceGroupB」のHCLファイルの例を示しましょう。\n最初は依存関係がないと仮定し、Aというクリーンなスレートのシナリオを考えます。BはAに依存するリソースグループとします。\n「ResourceGroupA」のHCLファイルの作成 vim envs/prod/ResourceGroupA/terragrunt.hcl env.hclから変数を受け取り、それらをモジュールに値として渡します。 locals { ENV = read_terragrunt_config(find_in_parent_folders(\u0026#34;env.hcl\u0026#34;)) } # Include definition of all environments (envs/terragrunt.hcl) include { path = find_in_parent_folders() } terraform { # Reference the module source = \u0026#34;../../../modules//ResourceGroupA\u0026#34; } # Specify the input values for the module inputs = { ENV = local.ENV.locals.ENV PROJECT_NAME = local.ENV.locals.PROJECT_NAME ACCOUNT_ID = local.ENV.locals.ACCOUNT_ID } 「ResourceGroupB」のファイルの作成： vim envs/prod/ResourceGroupB/terragrunt.hcl env.hclから変数を取得し、それらをモジュールに渡します。 この例では、ResourceGroupAで作成したVPCのIDをResourceGroupBに渡すことを想定しています。 locals { ENV = read_terragrunt_config(find_in_parent_folders(\u0026#34;env.hcl\u0026#34;)) } # Include definition of all environments (envs/terragrunt.hcl) include { path = find_in_parent_folders() } terraform { # Reference the module source = \u0026#34;../../../modules//ResourceGroupB\u0026#34; } # Specify the input values for the module inputs = { ENV = local.ENV.locals.ENV PROJECT_NAME = local.ENV.locals.PROJECT_NAME ACCOUNT_ID = local.ENV.locals.ACCOUNT_ID VPC_ID = local.ENV.locals.VPC_ID } モジュールのtfファイル 一般的には、通常どおりTerraformのコードを記述できます。 他のモジュールからパラメータを受け取りたい場合（依存関係がある場合やenv.hclからパラメータを渡す場合）、変数ブロックで空の変数を定義する必要があります。上記で言及したResourceGroupBの例では、次のようになります。 variable \u0026#34;ENV\u0026#34; { description = \u0026#34;Environment\u0026#34; type = string } variable \u0026#34;PROJECT_NAME\u0026#34; { description = \u0026#34;Project Name\u0026#34; type = string } variable \u0026#34;vpc_id\u0026#34; { description = \u0026#34;The ID of the VPC\u0026#34; type = string } 1つのリソースグループから別のリソースグループにパラメータを渡す場合は、それらを出力として定義する必要があります。例えば、Resource Group Aの場合、以下のように定義する必要があります： output \u0026#34;vpc_id\u0026#34; { value = aws_vpc.main.id } Deployment Terragrunt Commands Formatting cd envs/prod terragrunt run-all hclfmt terragrunt run-all fmt validate cd envs/prod terragrunt validate-all plan cd envs/prod terragrunt run-all plan apply cd envs/prod terragrunt run-all apply Dependency Graph tips Installing dot command on M1 Mac: Graphviz is a graph visualization tool that includes the dot command. You can install Graphviz by running the following command in the terminal: brew install graphviz Verify if the dot command is installed: dot -V cd envs/terragrunt-prod terragrunt graph-dependencies | dot -Tpng \u0026gt; graph-dependencies.png Deployment from git GithubAction name: Terragrunt Actions on: push: branches: - master env: TERRAGRUNT_CACHE_DIR: ${{ github.workspace }}/tool jobs: build: runs-on: ubuntu-latest env: TARGET_ENV: \u0026#39;\u0026#39; steps: - name: Set Production run: | mkdir -p ${GITHUB_WORKSPACE}/deploy echo \u0026#39;export TARGET_ENV=\u0026#34;prod\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${GITHUB_WORKSPACE}/deploy/.env if: github.ref == \u0026#39;refs/heads/master\u0026#39; env: GITHUB_WORKSPACE: ${{ github.workspace }} - name: Set Staging run: | mkdir -p ${GITHUB_WORKSPACE}/deploy echo \u0026#39;export TARGET_ENV=\u0026#34;stg\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${GITHUB_WORKSPACE}/deploy/.env if: github.ref == \u0026#39;refs/heads/staging\u0026#39; env: GITHUB_WORKSPACE: ${{ github.workspace }} - name: Terragrunt Plan env: TERRAGRUNT_DOWNLOAD: \u0026#34;https://github.com/gruntwork-io/terragrunt/releases/download/v0.34.0/terragrunt_linux_amd64\u0026#34; run: | source ${GITHUB_WORKSPACE}/deploy/.env sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y curl unzip git curl -Lo /tmp/terragrunt.zip ${TERRAGRUNT_DOWNLOAD} sudo unzip -d /usr/local/bin /tmp/terragrunt.zip cd ${GITHUB_WORKSPACE}/envs/${TARGET_ENV} terragrunt run-all plan if: github.ref == \u0026#39;refs/heads/master\u0026#39; || github.ref == \u0026#39;refs/heads/staging\u0026#39; env: GITHUB_WORKSPACE: ${{ github.workspace }} TERRAGRUNT_CACHE_DIR: ${{ env.TERRAGRUNT_CACHE_DIR }} - name: Terragrunt Apply if: github.ref == \u0026#39;refs/heads/master\u0026#39; env: TERRAGRUNT_DOWNLOAD: \u0026#34;https://github.com/gruntwork-io/terragrunt/releases/download/v0.34.0/terragrunt_linux_amd64\u0026#34; run: | source ${GITHUB_WORKSPACE}/deploy/.env sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y curl unzip git curl -Lo /tmp/terragrunt.zip ${TERRAGRUNT_DOWNLOAD} sudo unzip -d /usr/local/bin /tmp/terragrunt.zip cd ${GITHUB_WORKSPACE}/envs/${TARGET_ENV} terragrunt run-all apply --terragrunt-non-interactive env: GITHUB_WORKSPACE: ${{ github.workspace }} TERRAGRUNT_CACHE_DIR: ${{ env.TERRAGRUNT_CACHE_DIR }} Bitbucket-Pipelines image: hashicorp/terraform:1.3.7 definitions: caches: terragrunt: ${BITBUCKET_CLONE_DIR}/tool steps: - step: \u0026amp;set-production name: Set Production script: - mkdir -p ${BITBUCKET_CLONE_DIR}/deploy - echo \u0026#39;export TARGET_ENV=\u0026#34;prod\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${BITBUCKET_CLONE_DIR}/deploy/.env artifacts: - deploy/** - step: \u0026amp;set-staging name: Set Staging script: - mkdir -p ${BITBUCKET_CLONE_DIR}/deploy - echo \u0026#39;export TARGET_ENV=\u0026#34;stg\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${BITBUCKET_CLONE_DIR}/deploy/.env artifacts: - deploy/** - step: \u0026amp;terragrunt-plan name: terragrunt Plan caches: - terragrunt script: - source ${BITBUCKET_CLONE_DIR}/deploy/.env - apk update \u0026amp;\u0026amp; apk add bash curl groff jq less unzip git - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tfenv ]; then git clone https://github.com/tfutils/tfenv.git ${BITBUCKET_CLONE_DIR}/tool/tfenv; fi - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tgenv ]; then git clone https://github.com/cunymatthieu/tgenv.git ${BITBUCKET_CLONE_DIR}/tool/tgenv; fi - export PATH=${BITBUCKET_CLONE_DIR}/tool/tfenv/bin:$PATH - export PATH=${BITBUCKET_CLONE_DIR}/tool/tgenv/bin:$PATH - cd ${BITBUCKET_CLONE_DIR}/envs/${TARGET_ENV} - tfenv install \u0026amp;\u0026amp; tgenv install - terragrunt run-all plan - step: \u0026amp;terragrunt-apply name: terragrunt Apply trigger: manual caches: - terragrunt script: - source ${BITBUCKET_CLONE_DIR}/deploy/.env - apk update \u0026amp;\u0026amp; apk add bash curl groff jq less unzip git - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tfenv ]; then git clone https://github.com/tfutils/tfenv.git ${BITBUCKET_CLONE_DIR}/tool/tfenv; fi - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tgenv ]; then git clone https://github.com/cunymatthieu/tgenv.git ${BITBUCKET_CLONE_DIR}/tool/tgenv; fi - export PATH=${BITBUCKET_CLONE_DIR}/tool/tfenv/bin:$PATH - export PATH=${BITBUCKET_CLONE_DIR}/tool/tgenv/bin:$PATH - cd ${BITBUCKET_CLONE_DIR}/envs/${TARGET_ENV} - tfenv install \u0026amp;\u0026amp; tgenv install - terragrunt run-all apply --terragrunt-non-interactive pipelines: default: - step: *set-production - step: *terragrunt-plan branches: master: - step: *set-production - step: *terragrunt-plan - step: \u0026lt;\u0026lt;: *terragrunt-apply deployment: production ","permalink":"https://h-neco.github.io/blog/cicd-terragrunt-1/","tags":["OIDC","CI/CD","terraform","terragrunt","aws"],"title":"TerraformのWrapperツール、Terragrunt導入の検証を行いました。"},{"categories":null,"contents":"Intro AmazonLinux2を使用してNATインスタンスを設定しました。構成プロセスにPackerとAnsibleを使用しました。\nAmazonLinux2を使用してNATインスタンスを設定します。 Amazonlinux2が選択する理由 Lambdaは外部から開始される着信接続をサポートしていないため、アクティブモードでFTPをサポートしません。 ECSは、プライベートIPアドレスの静的割り当てを許可していません。 Gatewayタイプであるため、NATプライベートゲートウェイと組み合わせることで静的割り当てを実現することは可能ですが、外部ソースからの着信接続を受け入れることはできません。 技術要素 Packer Ansible iptables CENTOS7（AmazonLinux2）では、デフォルトのファイアウォール管理システムはファイアウォールです。ただし、IPTablesを紹介して、NATインスタンスをセットアップします。 ファイル構造 $ tree . . ├── ansible.cfg ├── bin │ └── init.sh ├── inventory │ └── hosts ├── packer-template │ └── nat_instance.json ├── playbook │ └── setup.yml └── roles └── iptable └── tasks ├── main.yml └── templates ├── iptables-config.j2 ├── nat_cidr.j2 └── sysctl.conf Packerとテンプレートファイルの実行 Execution Command packer build packer-template/nat_instance.json . └── packer-template └── nat_instance.json { \u0026#34;variables\u0026#34;: { \u0026#34;aws_access_key\u0026#34;: \u0026#34;{{env `AWS_ACCESS_KEY`}}\u0026#34;, \u0026#34;aws_secret_key\u0026#34;: \u0026#34;{{env `AWS_SECRET_KEY`}}\u0026#34; }, \u0026#34;builders\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;amazon-ebs\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;{{user `aws_access_key`}}\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;{{user `aws_secret_key`}}\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;ami_regions\u0026#34;: [ \u0026#34;ap-northeast-1\u0026#34; ], \u0026#34;associate_public_ip_address\u0026#34;: true, \u0026#34;source_ami\u0026#34;: \u0026#34;ami-0a3d21ec6281df8cb\u0026#34;, \u0026#34;instance_type\u0026#34;: \u0026#34;t3.micro\u0026#34;, \u0026#34;ssh_username\u0026#34;: \u0026#34;ec2-user\u0026#34;, \u0026#34;ami_name\u0026#34;: \u0026#34;nat-{{timestamp}}\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;nat-instance\u0026#34; }, \u0026#34;ena_support\u0026#34;: true, \u0026#34;enable_t2_unlimited\u0026#34;: false } ], \u0026#34;provisioners\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;./bin/init.sh\u0026#34;, \u0026#34;pause_before\u0026#34;: \u0026#34;60s\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;ansible-local\u0026#34;, \u0026#34;inventory_file\u0026#34;: \u0026#34;inventory/hosts\u0026#34;, \u0026#34;playbook_file\u0026#34;: \u0026#34;playbook/setup.yml\u0026#34;, \u0026#34;role_paths\u0026#34;: [ \u0026#34;roles/iptable\u0026#34; ], \u0026#34;staging_directory\u0026#34;: \u0026#34;/tmp/ansible-local\u0026#34;, \u0026#34;extra_arguments\u0026#34;: [\u0026#34;-vv\u0026#34;] }] } Ansibleの準備 ファイル構成 . ├── ansible.cfg ├── bin │ └── init.sh └── inventory └── hosts Install ansible Ansibleインストールは、 。/bin/init.shを実行することで実行できます\n#!/bin/bash sudo yum -y update # ansible install amazon-linux-extras install epel amazon-linux-extras enable ansible2 amazon-linux-extras install ansible2 インベントリファイルの構成 inventory/hosts ファイルを配置します。\n設定したサーバーからローカルでAnsibleを実行するため、ターゲットとして「ローカル」を指定します。\n[default] 127.0.0.1 構成ファイルを配置します ansible.cfgファイルを配置します\nこのファイルは、Ansibleの実行に使用されます。以下はサンプル構成です。\n[defaults] # Specify the Python interpreter interpreter_python=/usr/bin/python3 # Location of the hosts file inventory = inventroy/hosts # Whether to validate host keys when connecting host_key_checking = True # Number of parallel processes to use for remote connections forks=5 # Path to the log file log_path=~/logs/ansible/ansible.log # Color settings nocolor=0 scp_if_ssh=True Ansible Playbookを作成します file Structure . ├── playbook │ └── setup.yml └── roles └── iptable └── tasks ├── main.yml └── templates ├── iptables-config.j2 ├── nat_cidr.j2 └── sysctl.conf Packerによって呼び出されるPlaybook/setup.ymlファイルを作成します。 iptablesのロールを呼び出し、ルーティング後に出口時にソースアドレス/ポート変換を実行するチェーンを渡します。\n--- - hosts: all become: yes become_user: root remote_user: ec2-user roles: - role: iptable vars: nat_cidr: 10.0.0.1/24 また、playbookから呼び出されるiptablesロールのmain.ymlファイルも書きます。主な構成は、iptablesのインストールと必要な設定ファイルの構成です。\nroles/iptable/tasks/main.yml --- - name: Install iptables-service yum: name=iptables-services state=latest - name: Enable port forwarding template: src=\u0026#34;sysctl.conf\u0026#34; dest=\u0026#34;/etc/sysctl.conf\u0026#34; owner=root group=root mode=0644 # IP tables - name: Make iptables file template: src=\u0026#34;{{ item }}.j2\u0026#34; dest=\u0026#34;/etc/sysconfig/{{ item }}\u0026#34; owner=root group=root mode=0600 with_items: - \u0026#34;nat_cidr\u0026#34; # IP tables config - name: Make iptables-config file template: src=\u0026#34;iptables-config.j2\u0026#34; dest=\u0026#34;/etc/sysconfig/iptables-config-nat\u0026#34; owner=root group=root mode=0600 - name: Install ftp command yum: name=ftp state=latest - name: Install tcpdump command yum: name=tcpdump state=latest - name: Install lftp command yum: name=lftp state=latest 上記で使用したテンプレート・ファイルを書き込んでいきます。 roles/iptable/tasks/templates/iptables-config.j2 roles/iptable/tasks/templates/sysctl.conf roles/iptable/tasks/templates/nat_cidr.j2 roles/iptable/tasks/templates/iptables-config.j2\nIPTABLES_MODULES=\u0026#34;nf_conntrack_ftp nf_nat_ftp\u0026#34; IPTABLES_MODULES_UNLOAD=\u0026#34;yes\u0026#34; IPTABLES_SAVE_ON_STOP=\u0026#34;no\u0026#34; IPTABLES_SAVE_ON_RESTART=\u0026#34;no\u0026#34; IPTABLES_SAVE_COUNTER=\u0026#34;no\u0026#34; IPTABLES_STATUS_NUMERIC=\u0026#34;yes\u0026#34; IPTABLES_STATUS_VERBOSE=\u0026#34;no\u0026#34; IPTABLES_STATUS_LINENUMBERS=\u0026#34;yes\u0026#34; roles/iptable/tasks/templates/sysctl.conf\nIPv4フォワーディングと呼ばれる機能を有効にするには、「/proc/sys/net/ipv4/ip_forward」ファイルの内容を「1」に設定する。\nnet.ipv4.ip_forward=1 roles/iptable/tasks/templates/nat_cidr.j2\n*filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] COMMIT *nat :POSTROUTING ACCEPT [0:0] -A POSTROUTING -s {{ nat_cidr }} -j MASQUERADE COMMIT *raw :PREROUTING ACCEPT [0:0] -A PREROUTING -p tcp --dport 21 -j CT --helper ftp COMMIT EC2のネットワーク設定を構成するには インスタンス構成で、NATインスタンスのElastic Network Interface (ENI)の宛先チェックを無効にします。\naws ec2 modify-instance-attribute \\ --no-source-dest-check \\ --instance-id ${INSTANCE_ID} サーバーに接続してiptablesを起動するには Ansibleを使って設定ファイルをデプロイし、iptablesを再起動するには\n# 設定ファイルをAnsibleで配布されているファイルで上書きする sudo cp /etc/sysconfig/iptables-config-nat /etc/sysconfig/iptables # 設定の永続化 sudo service iptables save # サービスを再起動する sudo systemctl restart iptables ","permalink":"https://h-neco.github.io/blog/aws-ec2-nat-instance/","tags":["nat","ec2","packer","ansible"],"title":"PackerとAnsibleを使用してAmazon Linux 2にNATインスタンスを作成します。"},{"categories":null,"contents":"BOSH (Bosh Outer SHell) \u0026ldquo;\u0026hellip; is an open source tool for release engineering, deployment, lifecycle management, and monitoring of distributed systems.\u0026rdquo; And it\u0026rsquo;s amazingly powerful. This examples uses BOSH to provision an Alassian vendor app running on JDK along with the support Postgres database and agents to support it. The releases manages the health of services and will automatically provision, start/stop processes across the various services.\n","permalink":"https://h-neco.github.io/projects/creations/bosh-agents/","tags":["DevOps","BOSH","Java","Atlassian Ecosystem","monit","python","xml/xslt","bash/shell","REST APIs"],"title":"BOSH release for Bamboo \u0026 Remote Agents"},{"categories":null,"contents":"Multiple plugins used by thousands of teams that provide enhanced functionality of Atlassian’s core products (primarily JIRA and Bamboo) to enrich CI/CD capabilities, DevOps automation, or productivity. Functionality spans user interface, web services and persistence.\n","permalink":"https://h-neco.github.io/projects/creations/marketplace/","tags":["Java","Spring","REST APIs","Javascript","Atlassian Developer Ecosystem","Bamboo","JIRA","Bitbucket","Confluence","DevOps"],"title":"Atlassian Marketplace Plugins"},{"categories":null,"contents":"Provides required dependencies and additional utilities to simplify and codify the process of building, testing and delivering Atlassian plugins all the way to the live marketplace. Executes integration/AUT level tests against all stated compatible versions for the productUploads generated artifact to Atlassian marketplaceProvides corresponding metadata indicating version, release notes, and compatibility\n","permalink":"https://h-neco.github.io/projects/creations/docker-marketplace/","tags":["Docker","Maven","Java","Python","REST APIs","Bash/Shell"],"title":"Docker image for Bitbucket CI/CD Pipelines  \"shipit\""},{"categories":null,"contents":"aws EFSバーストモードクレジット枯渇対策メモ 2023-05-03 AWS CLIを使ったEC2へのEBSボリュームの永続化 2023-05-01 Lambdaローカル開発環境構築メモ (SAM | LocalStack | TypeScript) 2023-04-24 Packer と Ansible を使って Amazon Linux 2 に NAT インスタンスを作成する 2023-04-11 ci/cd Terraformのラッパーツール、Terragruntの導入検証を行ってみた 2023-04-27 OIDC (Git to AWS)のデプロイメモ 2023-04-26 ","permalink":"https://h-neco.github.io/blog/list/","tags":[],"title":"List"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"https://h-neco.github.io/search/","tags":null,"title":"Search Results"}]