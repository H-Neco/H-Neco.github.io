[{"categories":null,"contents":"Addressed pretty significant page load performance issue founde in larger deployments. Eliminates uses of intensive backend query, replacing it with an asynchronous API call against a lucene index. This change reduces page load from from 2+ minutes to nearly instant, with an incredibly responsive UI.\n","permalink":"https://h-neco.github.io/projects/contributions/deploy-triggers/","tags":["Java","jQuery","REST APIs","Bamboo","JSON"],"title":"Atlassian Deployment Triggers"},{"categories":null,"contents":"This talk looked at Liberty Mutual’s transformation to Continuous Integration, Continuous Delivery, and DevOps. For a large, heavily regulated industry, this task can not only be daunting, but viewed by many as impossible. Often, organizations try to reduce the friction through micro-fixes, but Eddie’s team asked how to change the culture to reduce the friction and concluded with the following final points:\nDon’t mandate DevOps. Give employees the chance to master their discipline with examples to set and follow. Favor deep end-to-end accomplishments over broad but incremental steps forward. Focus on taking the right teams far before encouraging broad adoption. Centralize the platforms and tools that your teams shouldn’t be thinking about. Provide foundational services/commodities and let teams stay on purpose. Incorporate contributions from everyone; don’t stifle autonomy. Stay open to new ways of working. Challenge security policies, but respect intentions. Find new ways to enforce concerns without abandoning precaution. ","permalink":"https://h-neco.github.io/publications/alldaydevops/","tags":["DevOps","Continuous Integration","Continuous Delivery","CI/CD pipelines","agile","Culture"],"title":"Organically DevOps: Building Quality and Security into the Software Supply Chain at Liberty Mutual"},{"categories":null,"contents":"Shields.io is a massive library of badges that can be inserted into project README\u0026rsquo;s or websites displaying various statuses (code coverage, health, version, etc). Support for docker was missing the current build health, and was a pretty trivial addition.\n","permalink":"https://h-neco.github.io/projects/contributions/shields-docker/","tags":["Docker","Rest APIs","JavaScript","node.js","JSON"],"title":"Added Docker Build Status Badge to shields.io"},{"categories":null,"contents":"While adding Structured Data to a client\u0026rsquo;s website I found some example JSON that was invalid. Simple contribution to cleanup the user documentation providing syntactically valid JSON documents.\n","permalink":"https://h-neco.github.io/projects/contributions/schema-org/","tags":["JSON"],"title":"Schema.org Structured Data documentation fixes"},{"categories":null,"contents":"Intro EFS Burst Mode Credit Exhaustion Countermeasures Memo. I will provide the countermeasures I took as a memo since I was using EFS in burst mode and my credits were running low. Technical Elements EFS aws Two Modes of EFS EFS has two modes: Burst Mode (General Purpose) and Provisioned Mode.\nBurst Mode automatically adjusts the traffic based on the amount of storage used and can handle temporary increases in traffic. The burstable bandwidth varies based on traffic usage and allows bursts as low as 105 Mbps. However, there is a limit on the number of read/write requests per second, so exceeding the limit may result in decreased throughput.\nProvisioned Mode allows you to set the volume\u0026rsquo;s throughput, minimum/maximum/burst, and specify write throughput. The read throughput is three times the write throughput. While Provisioned Mode does not have automatic scaling based on traffic, it can accommodate the required traffic based on the throughput settings.\nNotes and Countermeasures for Burst Mode (General Purpose) Note 1: Burst Credits\nBurst Mode consumes credits accumulated from file read/write operations and replenishes the credits based on the data usage on the NAS. The minimum speed of Burst Mode is 105 Mbps. However, when burst credits are exhausted, the performance of throughput significantly decreases, and file references from the mounting system may time out. How to check: Metric Name: BurstCreditBalance Initially, 2.3T credits are provided. To avoid exhausting burst credits, you can take the following countermeasures: Place a large amount of data to accelerate credit recovery. Switch to Provisioned Mode to ensure a constant throughput. Utilize caching mechanisms like CloudFront. Note 2: Request Limits\nBurst Mode has limits on the number of requests. There is a maximum limit on IOPS, which is 35,000 for reads and 7,000 for writes. If the maximum IOPS in Burst Mode is exceeded, the performance of throughput decreases. Therefore, if you require high IOPS, consider using Provisioned Mode. How to check: Metric Name: PercentIOLimit Countermeasures Taken Based on Consideration I adopted a strategy of placing a large amount of data in Burst Mode to accelerate the credit recovery speed.\nAttempting to place a 10GB file. Cost: $3/month Command: $ dd if=/dev/zero of=10GB_file_1 bs=10240k count=1000 How to check: Metric Name: BurstCreditBalance Provisioned Mode is not used due to high cost.\nThroughput 33(Mib/s) / Max Read Throughput 99(Mib/s) Cost: $238/month Throughput 15(Mib/s) / Max Read Throughput 45(Mib/s) Cost: $108/month Throughput 5(Mib/s) / Max Read Throughput 15(Mib/s) Cost: $36/month ","permalink":"https://h-neco.github.io/blog/aws-efs/","tags":["efs","aws"],"title":"EFS Burst Mode Credit Exhaustion Countermeasures Memo."},{"categories":null,"contents":"Intro By default, when an EC2 instance is terminated, the attached EBS volumes are also deleted. However, let\u0026rsquo;s explore how to persist them using AWS CLI. Technical Elements EC2/EBS Commands Retrieve the volume information of the target instance filtered by tags. If DeleteOnTermination is true, it means the volume is not persisted. $ aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=xxxxx-prod-web01\u0026#34; | jq -r .Reservations[0].Instances[0].BlockDeviceMappings [ { \u0026#34;DeviceName\u0026#34;: \u0026#34;/dev/sda1\u0026#34;, \u0026#34;Ebs\u0026#34;: { \u0026#34;AttachTime\u0026#34;: \u0026#34;2023-04-18T04:59:14+00:00\u0026#34;, \u0026#34;DeleteOnTermination\u0026#34;: true, \u0026#34;Status\u0026#34;: \u0026#34;attached\u0026#34;, \u0026#34;VolumeId\u0026#34;: \u0026#34;vol-xxxxxxxxxxxx\u0026#34; } } ] Prepare a configuration file. Set DeleteOnTermination to false. $ vim mapping.json [ { \u0026#34;DeviceName\u0026#34;: \u0026#34;/dev/sda1\u0026#34;, \u0026#34;Ebs\u0026#34;: { \u0026#34;DeleteOnTermination\u0026#34;: false } } ] Modify the instance attribute. $ aws ec2 modify-instance-attribute --instance-id \u0026#34;i-xxxxxxxxxxxxxx\u0026#34; --block-device-mappings file://mapping.json Retrieve the volume information of the target instance filtered by tags. Confirm that DeleteOnTermination is false. $ aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=xxxxx-prod-web01\u0026#34; | jq -r .Reservations[0].Instances[0].BlockDeviceMappings [ { \u0026#34;DeviceName\u0026#34;: \u0026#34;/dev/sda1\u0026#34;, \u0026#34;Ebs\u0026#34;: { \u0026#34;AttachTime\u0026#34;: \u0026#34;2023-04-18T04:59:14+00:00\u0026#34;, \u0026#34;DeleteOnTermination\u0026#34;: false, \u0026#34;Status\u0026#34;: \u0026#34;attached\u0026#34;, \u0026#34;VolumeId\u0026#34;: \u0026#34;vol-xxxxxxxxxxxx\u0026#34; } } ] ","permalink":"https://h-neco.github.io/blog/aws-ec2-ebs-delete-on-termination/","tags":["ec2","aws-cli","aws"],"title":"Persisting an Attached EBS Volume to EC2 Using AWS CLI."},{"categories":null,"contents":"Intro Previously, when deploying from GitHub/Bitbucket to personal AWS, I used access keys and secret keys. However, it became cumbersome to manage, so I switched to OIDC.\nTasks Set up OIDC with Terraform GitHub Bitbucket Deploy using OIDC Create simple GitHub Actions and Bitbucket Pipelines Technical Elements OIDC AWS Bitbucket GitHub Terraform What is OIDC? OIDC (OpenID Connect) is an authentication protocol that extends the OAuth 2.0 protocol and provides a mechanism for user authentication in web applications and mobile applications.\nBenefits of OIDC By using OIDC for deployment from Git, there are several benefits, including enhanced security for deployment through Git, easier management of token expiration, and the avoidance of security risks associated with using access keys without MFA enabled. However, using access keys with MFA enabled can be complicated, as it requires considering factors such as device management. With OIDC, users can use their ID provider\u0026rsquo;s authentication credentials, simplifying credential management and improving security.\nBackground Deploying to AWS used to be cumbersome. It even made me consider deploying from the management console instead of using the CLI.\nThe previous flow involved the inconvenience of obtaining keys: Checking MFA tokens from an app and executing a command: $ aws sts get-session-token --serial-number arn:aws:iam::xxxxxx:mfa/xxxxxx --token-code xxxxxx Exporting the obtained keys or registering them in Git Regularly executing commands due to session expiration\u0026hellip; Building OIDC with Terraform Building with Terraform Two scenarios will be explained: using GitHub Actions and using Bitbucket Pipelines. GitHub Sample variable \u0026#34;aws_account_id\u0026#34; {} variable \u0026#34;github_repo_name\u0026#34; {} variable \u0026#34;oidc_token_url\u0026#34; { default = \u0026#34;https://token.actions.githubusercontent.com\u0026#34; } data \u0026#34;tls_certificate\u0026#34; \u0026#34;github_oidc_token\u0026#34; { url = var.oidc_token_url } resource \u0026#34;aws_iam_openid_connect_provider\u0026#34; \u0026#34;github_oidc_provider\u0026#34; { url = var.oidc_token_url client_id_list = [ \u0026#34;sts.amazonaws.com\u0026#34; ] thumbprint_list = [data.tls_certificate.github_oidc_token.certificates.0.sha1_fingerprint] } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;github_oidc_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Federated\u0026#34; identifiers = [\u0026#34;arn:aws:iam::${var.aws_account_id}:oidc-provider/oidc-provider/token.actions.githubusercontent.com\u0026#34;] } condition { test = \u0026#34;StringEquals\u0026#34; variable = \u0026#34;token.actions.githubusercontent.com:sub\u0026#34; values = [\u0026#34;repo:${var.github_repo_name}:ref:refs/heads/main\u0026#34;] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;github_oidc_role\u0026#34; { name = \u0026#34;GithubOIDC-TEST\u0026#34; description = \u0026#34;GithubOIDC-TEST\u0026#34; assume_role_policy = data.aws_iam_policy_document.github_oidc_policy.json } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;github_oidc_administrator_access_attachment\u0026#34; { role = aws_iam_role.github_oidc_role.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/AdministratorAccess\u0026#34; } Bitbucket Sample variable \u0026#34;bitbucket_oidc_url\u0026#34; {} variable \u0026#34;bitbucket_oidc_audience\u0026#34; {} variable \u0026#34;account_id\u0026#34; {} variable \u0026#34;git_space\u0026#34; {} data \u0026#34;tls_certificate\u0026#34; \u0026#34;bitbucket\u0026#34; { url = var.bitbucket_oidc_url } resource \u0026#34;aws_iam_openid_connect_provider\u0026#34; \u0026#34;bitbucket\u0026#34; { url = var.bitbucket_oidc_url client_id_list = [ var.bitbucket_oidc_audience, ] thumbprint_list = [data.tls_certificate.bitbucket.certificates.0.sha1_fingerprint] } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;bitbucket_oidc_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Federated\u0026#34; identifiers = [\u0026#34;arn:aws:iam::${var.account_id}:oidc-provider/api.bitbucket.org/2.0/workspaces/${var.git_space}/pipelines-config/identity/oidc\u0026#34;] } condition { test = \u0026#34;StringEquals\u0026#34; variable = \u0026#34;api.bitbucket.org/2.0/workspaces/${var.git_space}/pipelines-config/identity/oidc:aud\u0026#34; values = [var.bitbucket_oidc_audience] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;bitbucket_oidc_role\u0026#34; { name = \u0026#34;BitbucketOIDC-TEST\u0026#34; description = \u0026#34;BitbucketOIDC-TEST\u0026#34; assume_role_policy = data.aws_iam_policy_document.bitbucket_oidc_policy.json } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;administrator_access_attachment\u0026#34; { role = aws_iam_role.bitbucket_oidc_role.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/AdministratorAccess\u0026#34; } Testing OIDC Setup Github Action The sample code deploys statically built Hugo content to S3 using OIDC. name: s3-deploy on: push: branches: - main jobs: s3put: runs-on: ubuntu-latest permissions: id-token: write contents: read steps: - name: Checkout code uses: actions/checkout@v2 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.111.3\u0026#39; - name: Build run: hugo --minify - uses: aws-actions/configure-aws-credentials@v1 with: aws-region: \u0026#39;ap-northeast-1\u0026#39; # Specify the region role-to-assume: \u0026#39;arn:aws:iam::xxxxxxxxxxx:role/oidc-role\u0026#39; # ARN of the created IAM role - name: Deploy run: aws s3 sync --delete public s3://my-s3-bucket/ Bitbucket Pipelines The sample code syncs the \u0026ldquo;public\u0026rdquo; folder from the master branch to my-s3-bucket using OIDC. image: amazon/aws-cli pipelines: default: - step: \u0026amp;s3-deploy name: Deploy to S3 with OIDC oidc: true script: - export AWS_WEB_IDENTITY_TOKEN_FILE=$(pwd)/web-identity-token - export AWS_ROLE_ARN=\u0026#39;created role\u0026#39; - echo $BITBUCKET_STEP_OIDC_TOKEN \u0026gt; $(pwd)/web-identity-token - aws s3 sync --delete public s3://my-s3-bucket/ branches: master: - step: *s3-deploy Setting up OIDC with CloudFormation Parameters: AwsAccountId: Type: String Description: AWS Account ID GithubRepoName: Type: String Description: Name of the GitHub repository OidcTokenUrl: Type: String Default: https://token.actions.githubusercontent.com Resources: GithubOidcTokenCertificate: Type: AWS::CloudFormation::CustomResource Version: \u0026#34;1.0\u0026#34; Properties: ServiceToken: !Sub arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:token-actions-github Url: !Ref OidcTokenUrl GithubOidcProvider: Type: AWS::IAM::OpenIDConnectProvider Properties: Url: !Ref OidcTokenUrl ClientIDList: - sts.amazonaws.com ThumbprintList: - !GetAtt GithubOidcTokenCertificate.Sha1Fingerprint GithubOidcPolicyDocument: Type: AWS::IAM::Policy Properties: PolicyName: GithubOIDCPolicy PolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: Allow Action: sts:AssumeRoleWithWebIdentity Resource: \u0026#34;*\u0026#34; Condition: StringEquals: token.actions.githubusercontent.com:sub: - !Sub \u0026#34;repo:${GithubRepoName}:ref:refs/heads/main\u0026#34; Principal: Federated: !Sub arn:aws:iam::${AwsAccountId}:oidc-provider/oidc-provider/token.actions.githubusercontent.com GithubOidcRole: Type: AWS::IAM::Role Properties: RoleName: GithubOIDC-TEST AssumeRolePolicyDocument: !Ref GithubOidcPolicyDocument GithubOidcAdministratorAccessAttachment: Type: AWS::IAM::PolicyAttachment Properties: PolicyArn: arn:aws:iam::aws:policy/AdministratorAccess Roles: - !Ref GithubOidcRole ","permalink":"https://h-neco.github.io/blog/cicd-oidc/","tags":["OIDC","CI/CD","github-action","bitbucket-pipelines","terraform"],"title":"Deployment Memo for OIDC (Git to AWS)"},{"categories":null,"contents":"Intro As part of replacing my PC, I rebuilt the local execution environment for Lambda. I\u0026rsquo;ll leave some notes as a memo. Rebuilt the local execution environment for Lambda while replacing my PC. I\u0026rsquo;ll leave some notes as a memo.\nTechnical Elements Volta Lambda LocalStack TypeScript SAM Prerequisites Installing Volta Installing Volta Volta is a tool specialized in managing Node.js versions. It allows you to switch between different Node.js versions for each project, making it easy to manage different versions without the need to install or manage Node.js versions manually. Volta provides the advantage of easy switching compared to other version management tools, and it can be used in various environments since it is not dependent on the operating system or shell.\n$ curl https://get.volta.sh | bash $ echo \u0026#39;export VOLTA_HOME=\u0026#34;$HOME/.volta\u0026#34;\u0026#39; \u0026gt;\u0026gt; .zshrc $ echo \u0026#39;export PATH=\u0026#34;$VOLTA_HOME/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; .zshrc Installing Node.js and Yarn You can use volta list node to check the available versions. If you want to install a specific version, use the format volta install node@18.15.0\n$ volta install node success: installed and set node@18.15.0 (with npm@9.5.0) as default $ volta install yarn success: installed and set yarn@4.0.0-rc.42 as default Installing SAM The following instructions are for macOS. For Windows users, it is recommended to download the MSI file from the official website.\nmacOS $ brew tap aws/tap $ brew install aws-sam-cli $ sam --version SAM CLI, version 1.78.0 Creating a Project $ sam init -r nodejs18.x SAM CLI now collects telemetry to better understand customer needs. You can OPT OUT and disable telemetry collection by setting the environment variable SAM_CLI_TELEMETRY=0 in your shell. Thanks for your help! Learn More: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-telemetry.html Which template source would you like to use? 1 - AWS Quick Start Templates 2 - Custom Template Location Choice: 1 Choose an AWS Quick Start application template 1 - Hello World Example 2 - Hello World Example With Powertools 3 - Multi-step workflow 4 - Standalone function 5 - Scheduled task 6 - Data processing 7 - Serverless API Template: 1 Based on your selections, the only Package type available is Zip. We will proceed to selecting the Package type as Zip. Based on your selections, the only dependency manager available is npm. We will proceed copying the template using npm. Select your starter template 1 - Hello World Example 2 - Hello World Example TypeScript Template: 2 Would you like to enable X-Ray tracing on the function(s) in your application? [y/N]: n Would you like to enable monitoring using CloudWatch Application Insights? For more info, please view https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.html [y/N]: n Project name [sam-app]: sam-local-study Cloning from https://github.com/aws/aws-sam-cli-app-templates (process may take a moment) ----------------------- Generating application: ----------------------- Name: sam-local-study Runtime: nodejs18.x Architectures: x86_64 Dependency Manager: npm Application Template: hello-world-typescript Output Directory: . Configuration file: sam-local-study/samconfig.toml Next steps can be found in the README file at sam-local-study/README.md Commands you can use next ========================= [*] Create pipeline: cd sam-local-study \u0026amp;\u0026amp; sam pipeline init --bootstrap [*] Validate SAM template: cd sam-local-study \u0026amp;\u0026amp; sam validate [*] Test Function in the Cloud: cd sam-local-study \u0026amp;\u0026amp; sam sync --stack-name {stack-name} --watch By following the prompts, the following files will be generated:\n$ tree . . └── sam-local-study ├── README.md ├── events │ └── event.json ├── hello-world │ ├── app.ts │ ├── jest.config.ts │ ├── package.json │ ├── tests │ │ └── unit │ │ └── test-handler.test.ts │ └── tsconfig.json ├── samconfig.toml └── template.yaml 6 directories, 9 files Creating a Sample Lambda Function That Returns a Message Let\u0026rsquo;s create a sample Lambda function that returns a message and execute it using SAM.\nCode Placement $ tree . . └── sam-local-study ├── src │ ├── handlers │ └── hello-from-lambda.js └── template.yaml src/handlers/hello-from-lambda.js exports.helloFromLambdaHandler = async () =\u0026gt; { const message = \u0026#39;Hello from Lambda!\u0026#39;; console.info(`${message}`); return message; } template.yaml Resources: HelloWorldFunction: Type: AWS::Serverless::Function Properties: Handler: src/handlers/hello-from-lambda.helloFromLambdaHandler Runtime: nodejs18.x MemorySize: 128 Timeout: 100 Description: A Lambda function that returns a static string. Policies: - AWSLambdaBasicExecutionRole Executing the Lambda Function That Returns \u0026ldquo;Hello\u0026rdquo; Execute the command: $ sam local invoke HelloWorldFunction Output: Invoking src/handlers/hello-from-lambda.helloFromLambdaHandler (nodejs18.x) Local image is up-to-date Using local image: public.ecr.aws/lambda/nodejs:18-rapid-x86_64. Mounting /lambda-sam-localstack/sam-local-study as /var/task:ro,delegated, inside runtime container START RequestId: 6d2615de-b38f-4300-bf87-704e1fa6296a Version: $LATEST 2023-04-05T08:41:27.339Z\t6d2615de-b38f-4300-bf87-704e1fa6296a\tINFO\tHello from Lambda! END RequestId: 6d2615de-b38f-4300-bf87-704e1fa6296a REPORT RequestId: 6d2615de-b38f-4300-bf87-704e1fa6296a\tInit Duration: 0.75 ms\tDuration: 953.12 ms\tBilled Duration: 954 ms\tMemory Size: 128 MB\tMax Memory Used: 128 MB \u0026#34;Hello from Lambda!\u0026#34;% Using LocalStack I will write about it at a later date\u0026hellip;.\n","permalink":"https://h-neco.github.io/blog/aws-lambda-local-execution/","tags":["volta","Lambda","LocalStack","TypeScript","SAM"],"title":"Lambda Local Development Environment Setup Memo (SAM | LocalStack | TypeScript)"},{"categories":null,"contents":"Intro I Conducted a Validation of Introducing Terragrunt, a Wrapper for Terraform. Introduction Validation Although I usually use Terraform, I found the following inconveniences: To minimize the impact scope of each deployment, I had to split Git repositories into smaller ones. Upgrading Terraform versions became challenging. It was difficult to understand resource dependencies from the code. There was a need to be mindful of the deployment order in areas where depends_on couldn\u0026rsquo;t be explicitly used. Challenges After Implementation The introduction is pending validation for the following two points: Not only managing Terraform but also managing Terragrunt versions. How does the version upgrade process change? If Terragrunt development stops, can we smoothly revert back to Terraform code? Technical Components terraform terragrunt aws Prerequisites What is terragrunt? Terragrunt is an open-source tool known as a Terraform Wrapper, designed to simplify the management of infrastructure using Terraform. It extends the capabilities provided by Terraform, allowing for code reuse, increased reusability, and flexibility in configuring modules. Terragrunt is especially useful when managing multiple environments or accounts using Terraform.\nInstallation $ brew install tfenv # Installs Terraform (version specified in .terraform-version) $ brew install tgenv # Installs Terragrunt (version specified in .terragrunt-version) Folder Structure This folder structure was created based on references from other articles. The advantage of this structure is that it allows for specifying different variables for each environment, resulting in increased code reusability.\n$ tree . . ├── README.md ├── docs │ └── graph-dependencies.png ├── envs │ ├── prod │ │ ├── ResourceGroupA │ │ │ └── terragrunt.hcl │ │ ├── ResourceGroupB │ │ │ └── terragrunt.hcl │ │ └── env.hcl │ └── terragrunt.hcl └── modules ├── ResourceGroupA │ └── xx.tf └── ResourceGroupB └── xx.tf Common File (envs/terragrunt.hcl) This file contains the configuration for storing *.tfstate files in an AWS S3 bucket as a backend, allowing each environment to have its own *.tfstate file stored within the S3 bucket. It also includes the provider definitions used by Terraform to create AWS resources. The configuration specifies different provider settings for each AWS region.\nBy managing the state of resources using *.tfstate files, conflicts can be avoided even when multiple people are working on the project.\n# Configuration for storing *.tfstate files for each environment remote_state { backend = \u0026#34;s3\u0026#34; config = { bucket = \u0026#34;tfstate-xxxxxxxxxxxxxx\u0026#34; # Stored in `stg/modA.tfstate` key = \u0026#34;${path_relative_to_include()}.tfstate\u0026#34; region = \u0026#34;ap-northeast-1\u0026#34; encrypt = true } generate = { path = \u0026#34;backend.tf\u0026#34; if_exists = \u0026#34;overwrite\u0026#34; } } generate \u0026#34;provider\u0026#34; { path = \u0026#34;provider.tf\u0026#34; if_exists = \u0026#34;overwrite_terragrunt\u0026#34; contents = \u0026lt;\u0026lt;EOF terraform { required_version = \u0026#34;\u0026gt;= 1.3.7\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.53.0\u0026#34; } } } # Tokyo region provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-1\u0026#34; alias = \u0026#34;tokyo\u0026#34; } # Virginia region provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; alias = \u0026#34;virginia\u0026#34; } # ... Other regions EOF } Environment-Specific File (envs/prod/env.hcl) This file is an environment-specific file for terragrunt, containing the configuration for the prod environment. These variables are used in other terragrunt files to define environment-specific settings. Specifically, they are called in the hcl file of envs/prod/ResourceGroupA and passed as parameters to the Terraform code in module/ResourceGroupA.\nlocals { ENV = \u0026#34;prod\u0026#34; PROJECT_NAME = \u0026#34;test\u0026#34; ACCOUNT_ID = \u0026#34;123456789\u0026#34; VPC_CIDE = \u0026#34;10.0.0.0/24\u0026#34; } HCL Files for Each Resource Grouping Let\u0026rsquo;s provide an example of the hcl files for \u0026ldquo;ResourceGroupA\u0026rdquo; and \u0026ldquo;ResourceGroupB\u0026rdquo;.\nAssuming there are no dependencies initially, we\u0026rsquo;ll consider a clean slate scenario called A, with B being a resource grouping that depends on A.\nCreating the hcl file for \u0026ldquo;ResourceGroupA\u0026rdquo;:\nvim envs/prod/ResourceGroupA/terragrunt.hcl Receive variables from env.hcl and pass them as values to the module. locals { ENV = read_terragrunt_config(find_in_parent_folders(\u0026#34;env.hcl\u0026#34;)) } # Include definition of all environments (envs/terragrunt.hcl) include { path = find_in_parent_folders() } terraform { # Reference the module source = \u0026#34;../../../modules//ResourceGroupA\u0026#34; } # Specify the input values for the module inputs = { ENV = local.ENV.locals.ENV PROJECT_NAME = local.ENV.locals.PROJECT_NAME ACCOUNT_ID = local.ENV.locals.ACCOUNT_ID } Creating the file for ResourceGroupB: vim envs/prod/ResourceGroupB/terragrunt.hcl Retrieve variables from env.hcl and pass values to the module. The example assumes passing the VPC ID created in ResourceGroupA to ResourceGroupB. locals { ENV = read_terragrunt_config(find_in_parent_folders(\u0026#34;env.hcl\u0026#34;)) } # Include definition of all environments (envs/terragrunt.hcl) include { path = find_in_parent_folders() } terraform { # Reference the module source = \u0026#34;../../../modules//ResourceGroupB\u0026#34; } # Specify the input values for the module inputs = { ENV = local.ENV.locals.ENV PROJECT_NAME = local.ENV.locals.PROJECT_NAME ACCOUNT_ID = local.ENV.locals.ACCOUNT_ID VPC_ID = local.ENV.locals.VPC_ID } Module\u0026rsquo;s tf file In general, you can write Terraform code as usual. If you want to receive parameters from other modules (in the case of dependencies or when passing parameters from env.hcl), you need to define empty variables in the variables block. In the example of ResourceGroupB mentioned above, it would look like the following: variable \u0026#34;ENV\u0026#34; { description = \u0026#34;Environment\u0026#34; type = string } variable \u0026#34;PROJECT_NAME\u0026#34; { description = \u0026#34;Project Name\u0026#34; type = string } variable \u0026#34;vpc_id\u0026#34; { description = \u0026#34;The ID of the VPC\u0026#34; type = string } If you want to pass parameters from one resource group to another, you need to define them as outputs. For example, in the case of Resource Group A, you would need to define them as follows: output \u0026#34;vpc_id\u0026#34; { value = aws_vpc.main.id } macOS $ brew tap aws/tap $ brew install aws-sam-cli $ sam --version SAM CLI, version 1.78.0 Deployment Terragrunt Commands Formatting: cd envs/prod terragrunt run-all hclfmt terragrunt run-all fmt validate cd envs/prod terragrunt validate-all plan cd envs/prod terragrunt run-all plan apply cd envs/prod terragrunt run-all apply Dependency Graph tips Installing dot command on M1 Mac: Graphviz is a graph visualization tool that includes the dot command. You can install Graphviz by running the following command in the terminal: brew install graphviz Verify if the dot command is installed: dot -V cd envs/terragrunt-prod terragrunt graph-dependencies | dot -Tpng \u0026gt; graph-dependencies.png Deployment from git GithubAction name: Terragrunt Actions on: push: branches: - master env: TERRAGRUNT_CACHE_DIR: ${{ github.workspace }}/tool jobs: build: runs-on: ubuntu-latest env: TARGET_ENV: \u0026#39;\u0026#39; steps: - name: Set Production run: | mkdir -p ${GITHUB_WORKSPACE}/deploy echo \u0026#39;export TARGET_ENV=\u0026#34;prod\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${GITHUB_WORKSPACE}/deploy/.env if: github.ref == \u0026#39;refs/heads/master\u0026#39; env: GITHUB_WORKSPACE: ${{ github.workspace }} - name: Set Staging run: | mkdir -p ${GITHUB_WORKSPACE}/deploy echo \u0026#39;export TARGET_ENV=\u0026#34;stg\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${GITHUB_WORKSPACE}/deploy/.env if: github.ref == \u0026#39;refs/heads/staging\u0026#39; env: GITHUB_WORKSPACE: ${{ github.workspace }} - name: Terragrunt Plan env: TERRAGRUNT_DOWNLOAD: \u0026#34;https://github.com/gruntwork-io/terragrunt/releases/download/v0.34.0/terragrunt_linux_amd64\u0026#34; run: | source ${GITHUB_WORKSPACE}/deploy/.env sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y curl unzip git curl -Lo /tmp/terragrunt.zip ${TERRAGRUNT_DOWNLOAD} sudo unzip -d /usr/local/bin /tmp/terragrunt.zip cd ${GITHUB_WORKSPACE}/envs/${TARGET_ENV} terragrunt run-all plan if: github.ref == \u0026#39;refs/heads/master\u0026#39; || github.ref == \u0026#39;refs/heads/staging\u0026#39; env: GITHUB_WORKSPACE: ${{ github.workspace }} TERRAGRUNT_CACHE_DIR: ${{ env.TERRAGRUNT_CACHE_DIR }} - name: Terragrunt Apply if: github.ref == \u0026#39;refs/heads/master\u0026#39; env: TERRAGRUNT_DOWNLOAD: \u0026#34;https://github.com/gruntwork-io/terragrunt/releases/download/v0.34.0/terragrunt_linux_amd64\u0026#34; run: | source ${GITHUB_WORKSPACE}/deploy/.env sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y curl unzip git curl -Lo /tmp/terragrunt.zip ${TERRAGRUNT_DOWNLOAD} sudo unzip -d /usr/local/bin /tmp/terragrunt.zip cd ${GITHUB_WORKSPACE}/envs/${TARGET_ENV} terragrunt run-all apply --terragrunt-non-interactive env: GITHUB_WORKSPACE: ${{ github.workspace }} TERRAGRUNT_CACHE_DIR: ${{ env.TERRAGRUNT_CACHE_DIR }} Bitbucket-Pipelines image: hashicorp/terraform:1.3.7 definitions: caches: terragrunt: ${BITBUCKET_CLONE_DIR}/tool steps: - step: \u0026amp;set-production name: Set Production script: - mkdir -p ${BITBUCKET_CLONE_DIR}/deploy - echo \u0026#39;export TARGET_ENV=\u0026#34;prod\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${BITBUCKET_CLONE_DIR}/deploy/.env artifacts: - deploy/** - step: \u0026amp;set-staging name: Set Staging script: - mkdir -p ${BITBUCKET_CLONE_DIR}/deploy - echo \u0026#39;export TARGET_ENV=\u0026#34;stg\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${BITBUCKET_CLONE_DIR}/deploy/.env artifacts: - deploy/** - step: \u0026amp;terragrunt-plan name: terragrunt Plan caches: - terragrunt script: - source ${BITBUCKET_CLONE_DIR}/deploy/.env - apk update \u0026amp;\u0026amp; apk add bash curl groff jq less unzip git - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tfenv ]; then git clone https://github.com/tfutils/tfenv.git ${BITBUCKET_CLONE_DIR}/tool/tfenv; fi - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tgenv ]; then git clone https://github.com/cunymatthieu/tgenv.git ${BITBUCKET_CLONE_DIR}/tool/tgenv; fi - export PATH=${BITBUCKET_CLONE_DIR}/tool/tfenv/bin:$PATH - export PATH=${BITBUCKET_CLONE_DIR}/tool/tgenv/bin:$PATH - cd ${BITBUCKET_CLONE_DIR}/envs/${TARGET_ENV} - tfenv install \u0026amp;\u0026amp; tgenv install - terragrunt run-all plan - step: \u0026amp;terragrunt-apply name: terragrunt Apply trigger: manual caches: - terragrunt script: - source ${BITBUCKET_CLONE_DIR}/deploy/.env - apk update \u0026amp;\u0026amp; apk add bash curl groff jq less unzip git - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tfenv ]; then git clone https://github.com/tfutils/tfenv.git ${BITBUCKET_CLONE_DIR}/tool/tfenv; fi - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tgenv ]; then git clone https://github.com/cunymatthieu/tgenv.git ${BITBUCKET_CLONE_DIR}/tool/tgenv; fi - export PATH=${BITBUCKET_CLONE_DIR}/tool/tfenv/bin:$PATH - export PATH=${BITBUCKET_CLONE_DIR}/tool/tgenv/bin:$PATH - cd ${BITBUCKET_CLONE_DIR}/envs/${TARGET_ENV} - tfenv install \u0026amp;\u0026amp; tgenv install - terragrunt run-all apply --terragrunt-non-interactive pipelines: default: - step: *set-production - step: *terragrunt-plan branches: master: - step: *set-production - step: *terragrunt-plan - step: \u0026lt;\u0026lt;: *terragrunt-apply deployment: production ","permalink":"https://h-neco.github.io/blog/cicd-terragrunt-1/","tags":["OIDC","CI/CD","terraform","terragrunt","aws"],"title":"I Conducted a Validation of Introducing Terragrunt, a Wrapper for Terraform."},{"categories":null,"contents":"Intro I have set up a NAT instance using amazonLinux2. I utilized Packer and Ansible for the configuration process.\nSet up a NAT instance using amazonLinux2. Explain why amazonLinux2 is chosen: Lambda does not support incoming connections initiated externally, so it does not support FTP in active mode. ECS does not allow for the static allocation of private IP addresses. While it is possible to achieve static allocation by combining it with NAT Private Gateway, since it is a gateway type, it cannot accept incoming connections from external sources. Technical Elements Packer Ansible iptables In CentOS7 (amazonLinux2), the default firewall management system is firewalld. However, I will introduce iptables to set up the NAT instance. File Structure $ tree . . ├── ansible.cfg ├── bin │ └── init.sh ├── inventory │ └── hosts ├── packer-template │ └── nat_instance.json ├── playbook │ └── setup.yml └── roles └── iptable └── tasks ├── main.yml └── templates ├── iptables-config.j2 ├── nat_cidr.j2 └── sysctl.conf Executing Packer and Template Files Execution Command packer build packer-template/nat_instance.json . └── packer-template └── nat_instance.json { \u0026#34;variables\u0026#34;: { \u0026#34;aws_access_key\u0026#34;: \u0026#34;{{env `AWS_ACCESS_KEY`}}\u0026#34;, \u0026#34;aws_secret_key\u0026#34;: \u0026#34;{{env `AWS_SECRET_KEY`}}\u0026#34; }, \u0026#34;builders\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;amazon-ebs\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;{{user `aws_access_key`}}\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;{{user `aws_secret_key`}}\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;ami_regions\u0026#34;: [ \u0026#34;ap-northeast-1\u0026#34; ], \u0026#34;associate_public_ip_address\u0026#34;: true, \u0026#34;source_ami\u0026#34;: \u0026#34;ami-0a3d21ec6281df8cb\u0026#34;, \u0026#34;instance_type\u0026#34;: \u0026#34;t3.micro\u0026#34;, \u0026#34;ssh_username\u0026#34;: \u0026#34;ec2-user\u0026#34;, \u0026#34;ami_name\u0026#34;: \u0026#34;nat-{{timestamp}}\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;nat-instance\u0026#34; }, \u0026#34;ena_support\u0026#34;: true, \u0026#34;enable_t2_unlimited\u0026#34;: false } ], \u0026#34;provisioners\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;./bin/init.sh\u0026#34;, \u0026#34;pause_before\u0026#34;: \u0026#34;60s\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;ansible-local\u0026#34;, \u0026#34;inventory_file\u0026#34;: \u0026#34;inventory/hosts\u0026#34;, \u0026#34;playbook_file\u0026#34;: \u0026#34;playbook/setup.yml\u0026#34;, \u0026#34;role_paths\u0026#34;: [ \u0026#34;roles/iptable\u0026#34; ], \u0026#34;staging_directory\u0026#34;: \u0026#34;/tmp/ansible-local\u0026#34;, \u0026#34;extra_arguments\u0026#34;: [\u0026#34;-vv\u0026#34;] }] } Preparing Ansible file Structure . ├── ansible.cfg ├── bin │ └── init.sh └── inventory └── hosts Install ansible Ansible installation can be done by running ./bin/init.sh\n#!/bin/bash sudo yum -y update # ansible install amazon-linux-extras install epel amazon-linux-extras enable ansible2 amazon-linux-extras install ansible2 Configuration of the inventory file Place the inventory/hosts file\nSince I will be running Ansible locally from the server I set up, specify \u0026ldquo;local\u0026rdquo; as the target.\n[default] 127.0.0.1 Placing the configuration file Place the ansible.cfg file\nThis file is used for running Ansible. Below is the sample configuration:\n[defaults] # Specify the Python interpreter interpreter_python=/usr/bin/python3 # Location of the hosts file inventory = inventroy/hosts # Whether to validate host keys when connecting host_key_checking = True # Number of parallel processes to use for remote connections forks=5 # Path to the log file log_path=~/logs/ansible/ansible.log # Color settings nocolor=0 scp_if_ssh=True Creating an Ansible Playbook file Structure . ├── playbook │ └── setup.yml └── roles └── iptable └── tasks ├── main.yml └── templates ├── iptables-config.j2 ├── nat_cidr.j2 └── sysctl.conf Writing playbook/setup.yml Invoked by Packer.I will be writing the playbook/setup.yml file that is invoked by Packer. I will call the iptables role and pass the chain that performs source address/port translation upon egress after routing.\n--- - hosts: all become: yes become_user: root remote_user: ec2-user roles: - role: iptable vars: nat_cidr: 10.0.0.1/24 I will also write the main.yml file for the iptables role that is invoked by the playbook. The main structure involves the installation of iptables and the configuration of the necessary configuration files.\nroles/iptable/tasks/main.yml --- - name: Install iptables-service yum: name=iptables-services state=latest - name: Enable port forwarding template: src=\u0026#34;sysctl.conf\u0026#34; dest=\u0026#34;/etc/sysctl.conf\u0026#34; owner=root group=root mode=0644 # IP tables - name: Make iptables file template: src=\u0026#34;{{ item }}.j2\u0026#34; dest=\u0026#34;/etc/sysconfig/{{ item }}\u0026#34; owner=root group=root mode=0600 with_items: - \u0026#34;nat_cidr\u0026#34; # IP tables config - name: Make iptables-config file template: src=\u0026#34;iptables-config.j2\u0026#34; dest=\u0026#34;/etc/sysconfig/iptables-config-nat\u0026#34; owner=root group=root mode=0600 - name: Install ftp command yum: name=ftp state=latest - name: Install tcpdump command yum: name=tcpdump state=latest - name: Install lftp command yum: name=lftp state=latest Writing the Template Files Used in the Above.We will proceed to write the template files used in the above instructions. roles/iptable/tasks/templates/iptables-config.j2 roles/iptable/tasks/templates/sysctl.conf roles/iptable/tasks/templates/nat_cidr.j2 roles/iptable/tasks/templates/iptables-config.j2\nIPTABLES_MODULES=\u0026#34;nf_conntrack_ftp nf_nat_ftp\u0026#34; IPTABLES_MODULES_UNLOAD=\u0026#34;yes\u0026#34; IPTABLES_SAVE_ON_STOP=\u0026#34;no\u0026#34; IPTABLES_SAVE_ON_RESTART=\u0026#34;no\u0026#34; IPTABLES_SAVE_COUNTER=\u0026#34;no\u0026#34; IPTABLES_STATUS_NUMERIC=\u0026#34;yes\u0026#34; IPTABLES_STATUS_VERBOSE=\u0026#34;no\u0026#34; IPTABLES_STATUS_LINENUMBERS=\u0026#34;yes\u0026#34; roles/iptable/tasks/templates/sysctl.conf\nTo enable a feature called IPv4 forwarding, set the contents of the \u0026ldquo;/proc/sys/net/ipv4/ip_forward\u0026rdquo; file to \u0026ldquo;1\u0026rdquo;.\nnet.ipv4.ip_forward=1 roles/iptable/tasks/templates/nat_cidr.j2\n*filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] COMMIT *nat :POSTROUTING ACCEPT [0:0] -A POSTROUTING -s {{ nat_cidr }} -j MASQUERADE COMMIT *raw :PREROUTING ACCEPT [0:0] -A PREROUTING -p tcp --dport 21 -j CT --helper ftp COMMIT To configure the network settings for EC2 Disable the destination check for the NAT instance\u0026rsquo;s Elastic Network Interface (ENI) in your instance configuration.\naws ec2 modify-instance-attribute \\ --no-source-dest-check \\ --instance-id ${INSTANCE_ID} To connect to the server and start iptables To deploy the configuration file using Ansible and restart iptables\n# Overwrite the configuration file with the file distributed through Ansible sudo cp /etc/sysconfig/iptables-config-nat /etc/sysconfig/iptables # Persist the configuration sudo service iptables save # Restart the service sudo systemctl restart iptables ","permalink":"https://h-neco.github.io/blog/aws-ec2-nat-instance/","tags":["nat","ec2","packer","ansible"],"title":"Creating a NAT instance on Amazon Linux 2 using Packer and Ansible."},{"categories":null,"contents":"BOSH (Bosh Outer SHell) \u0026ldquo;\u0026hellip; is an open source tool for release engineering, deployment, lifecycle management, and monitoring of distributed systems.\u0026rdquo; And it\u0026rsquo;s amazingly powerful. This examples uses BOSH to provision an Alassian vendor app running on JDK along with the support Postgres database and agents to support it. The releases manages the health of services and will automatically provision, start/stop processes across the various services.\n","permalink":"https://h-neco.github.io/projects/creations/bosh-agents/","tags":["DevOps","BOSH","Java","Atlassian Ecosystem","monit","python","xml/xslt","bash/shell","REST APIs"],"title":"BOSH release for Bamboo \u0026 Remote Agents"},{"categories":null,"contents":"Multiple plugins used by thousands of teams that provide enhanced functionality of Atlassian’s core products (primarily JIRA and Bamboo) to enrich CI/CD capabilities, DevOps automation, or productivity. Functionality spans user interface, web services and persistence.\n","permalink":"https://h-neco.github.io/projects/creations/marketplace/","tags":["Java","Spring","REST APIs","Javascript","Atlassian Developer Ecosystem","Bamboo","JIRA","Bitbucket","Confluence","DevOps"],"title":"Atlassian Marketplace Plugins"},{"categories":null,"contents":"Provides required dependencies and additional utilities to simplify and codify the process of building, testing and delivering Atlassian plugins all the way to the live marketplace. Executes integration/AUT level tests against all stated compatible versions for the productUploads generated artifact to Atlassian marketplaceProvides corresponding metadata indicating version, release notes, and compatibility\n","permalink":"https://h-neco.github.io/projects/creations/docker-marketplace/","tags":["Docker","Maven","Java","Python","REST APIs","Bash/Shell"],"title":"Docker image for Bitbucket CI/CD Pipelines  \"shipit\""},{"categories":null,"contents":"aws EFS Burst Mode Credit Exhaustion Countermeasures Memo. 2023-05-03 Persisting an Attached EBS Volume to EC2 Using AWS CLI. 2023-05-01 Lambda Local Development Environment Setup Memo (SAM | LocalStack | TypeScript). 2023-04-24 Creating a NAT instance on Amazon Linux 2 using Packer and Ansible. 2023-04-11 ci/cd I Conducted a Validation of Introducing Terragrunt, a Wrapper for Terraform. 2023-04-27 Deployment Memo for OIDC (Git to AWS). 2023-04-26 ","permalink":"https://h-neco.github.io/blog/list/","tags":[],"title":"List"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"https://h-neco.github.io/search/","tags":null,"title":"Search Results"}]