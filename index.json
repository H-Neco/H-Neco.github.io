[{"categories":null,"contents":"Intro 以前は、vagrant+virtualbox で仮想マシンを作成してましたが、m2 mac に端末を変えたため、multipass に切り替えました。\nmultipass は、Ubuntu の仮想マシンを簡単に作成できるツールです。multipass を使用することで、仮想マシンの作成、起動、停止、削除などの操作を簡単に行うことができます。\ninstall $ brew install multipass ==\u0026gt; Installing Cask multipass ==\u0026gt; Running installer for multipass with sudo; the password may be necessary. Password: installer: Package name is multipass installer: Installing at base path / installer: The install was successful. 🍺 multipass was successfully installed! $ multipass version multipass 1.13.1+mac multipassd 1.13.1+mac 基本コマンド 詳しくは以下参照 https://multipass.run/docs/multipass-cli-client # 新規作成 $ multipass launch -n test-vm-01 Launched: test-vm-01 # 確認 $ multipass list Name State IPv4 Image test-vm-01 Running 192.168.64.2 Ubuntu 22.04 LTS $ multipass info test-vm-01 Name: test-vm-01 State: Running Snapshots: 0 IPv4: 192.168.64.2 Release: Ubuntu 22.04.4 LTS Image hash: 40ea1181447b (Ubuntu 22.04 LTS) CPU(s): 1 Load: 0.07 0.04 0.01 Disk usage: 1.6GiB out of 4.8GiB Memory usage: 150.2MiB out of 962.3MiB Mounts: -- # 停止 $ multipass stop test-vm-01 # 起動 $ multipass start test-vm-01 # 削除 (-p で永続化) $ multipass delete test-vm-01 -p # シェルに入る $ multipass shell test-vm-01 docker 環境構築 # 鍵作成 $ ssh-keygen -t rsa -b 4096 -C \u0026#34;\u0026#34; -f multipass # cloud-init 作成 $ vim multipass.yml #!/bin/sh AUTHORIZED_KEYS=$(cat multipass.pub ) cat \u0026gt; ./multipass_docker.yaml \u0026lt;\u0026lt; _EOF_ --- locale: en_US.UTF8 timezone: Asia/Tokyo package_upgrade: true users: - name: ubuntu sudo: ALL=(ALL) NOPASSWD:ALL ssh-authorized-keys: - ${AUTHORIZED_KEYS} packages: - docker - docker-compose - avahi-daemon - apt-transport-https - ca-certificates - curl - gnupg - lsb-release runcmd: - sudo curl -fsSL https://get.docker.com | sudo bash - sudo systemctl enable docker - sudo systemctl enable -s HUP ssh - sudo groupadd docker - sudo usermod -aG docker ubuntu _EOF_ # 起動 $ multipass launch -n test-vm-01 --cloud-init multipass.yml # 確認 $ multipass exec test-vm-01 docker version Client: Docker Engine - Community Version: 25.0.5 ... ","permalink":"https://h-neco.github.io/blog/vm-multipass/","tags":["Tools"],"title":"multipassでの仮想マシンの作成/Docker環境構築"},{"categories":null,"contents":"Intro trivy は、コンテナイメージの脆弱性診断を行うツールです。trivy を使用することで、コンテナイメージに含まれる脆弱性を検出し、セキュリティを向上させることができます。\ndocker の環境構築についてはこちら 環境構築 作業の大枠 Dockerfile に対して trivy で診断を行います。 問題なければ、イメージをビルドし、trivy で再度スキャンを行います。 事前準備 適当なディレクトリを作成し、Dockerfile を作成します。 $ mkdir test $ vim test/Dockerfile FROM public.ecr.aws/nginx/nginx:1.25-alpine-slim EXPOSE 80/tcp 診断 DockerFile を trivy で診断します。config コマンドを実行します。 $ docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd)/test:/workdir aquasec/trivy config --ignorefile .trivy/.trivyignore --severity HIGH,CRITICAL . 2024-03-20T13:16:35.209Z\tINFO\tMisconfiguration scanning is enabled 2024-03-20T13:16:35.210Z\tINFO\tNeed to update the built-in policies 2024-03-20T13:16:35.211Z\tINFO\tDownloading the built-in policies... 46.13 KiB / 46.13 KiB [-------------------------------------------------------------------------------------------------------------------------------------------------] 100.00% ? p/s 100ms 2024-03-20T13:16:36.947Z\tINFO\tDetected config files: 1 Dockerfile (dockerfile) Tests: 20 (SUCCESSES: 19, FAILURES: 1, EXCEPTIONS: 0) Failures: 1 (HIGH: 1, CRITICAL: 0) HIGH: Specify at least 1 USER command in Dockerfile with non-root user as argument ═════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════════ Running containers with \u0026#39;root\u0026#39; user can lead to a container escape situation. It is a best practice to run containers as non-root users, which can be done by adding a \u0026#39;USER\u0026#39; statement to the Dockerfile. See https://avd.aquasec.com/misconfig/ds002 ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── root ユーザでの実行が検出されたので修正してみます。 $ vim test/Dockerfile FROM public.ecr.aws/nginx/nginx:1.25-alpine-slim RUN apk add --no-cache shadow \u0026amp;\u0026amp; \\ useradd -u 9000 test USER test # 再確認 docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd)/test:/workdir aquasec/trivy config --ignorefile .trivy/.trivyignore --severity HIGH,CRITICAL . 2024-03-20T13:28:55.680Z\tINFO\tMisconfiguration scanning is enabled 2024-03-20T13:28:55.681Z\tINFO\tNeed to update the built-in policies 2024-03-20T13:28:55.682Z\tINFO\tDownloading the built-in policies... 46.13 KiB / 46.13 KiB [-------------------------------------------------------------------------------------------------------------------------------------------------] 100.00% ? p/s 100ms 2024-03-20T13:28:57.980Z\tINFO\tDetected config files: 1 検出しなくなったので build します。 $ cd test \u0026amp;\u0026amp; docker build -f Dockerfile -t test-nginx-01:latest . イメージを trivy でスキャンします。image コマンドを実行します。 ubuntu@test-vm-01:~$ docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd)/test:/workdir aquasec/trivy image test-nginx-01:latest 2024-03-20T13:39:44.909Z\tINFO\tNeed to update DB 2024-03-20T13:39:44.909Z\tINFO\tDB Repository: ghcr.io/aquasecurity/trivy-db:2 2024-03-20T13:39:44.909Z\tINFO\tDownloading DB... 1.21 MiB / 44.46 MiB [-\u0026gt;_____________________________________________________________] 2.71% ? p/s ?2.38 MiB / 44.46 MiB [---\u0026gt;___________________________________________________________] 5.36% ? p/s ?3.81 MiB / 44.46 MiB [-----\u0026gt;_________________________________________________________] 8.58% ? p/s ?4.92 MiB / 44.46 MiB [-----\u0026gt;____________________________________________] 11.06% 6.17 MiB p/s ETA 6s6.23 MiB / 44.46 MiB [-------\u0026gt;__________________________________________] 14.00% 6.17 MiB p/s ETA 6s7.35 MiB / 44.46 MiB [--------\u0026gt;_________________________________________] 16.54% 6.17 MiB p/s ETA 6s9.36 MiB / 44.46 MiB [----------\u0026gt;_______________________________________] 21.05% 6.25 MiB p/s ETA 5s11.68 MiB / 44.46 MiB [------------\u0026gt;____________________________________] 26.28% 6.25 MiB p/s ETA 5s14.02 MiB / 44.46 MiB [---------------\u0026gt;_________________________________] 31.52% 6.25 MiB p/s ETA 4s16.96 MiB / 44.46 MiB [------------------\u0026gt;______________________________] 38.14% 6.67 MiB p/s ETA 4s20.00 MiB / 44.46 MiB [----------------------\u0026gt;__________________________] 44.98% 6.67 MiB p/s ETA 3s22.54 MiB / 44.46 MiB [------------------------\u0026gt;________________________] 50.69% 6.67 MiB p/s ETA 3s25.50 MiB / 44.46 MiB [----------------------------\u0026gt;____________________] 57.36% 7.16 MiB p/s ETA 2s28.53 MiB / 44.46 MiB [-------------------------------\u0026gt;_________________] 64.18% 7.16 MiB p/s ETA 2s30.86 MiB / 44.46 MiB [----------------------------------\u0026gt;______________] 69.41% 7.16 MiB p/s ETA 1s33.75 MiB / 44.46 MiB [-------------------------------------\u0026gt;___________] 75.91% 7.58 MiB p/s ETA 1s36.58 MiB / 44.46 MiB [----------------------------------------\u0026gt;________] 82.27% 7.58 MiB p/s ETA 1s39.46 MiB / 44.46 MiB [-------------------------------------------\u0026gt;_____] 88.77% 7.58 MiB p/s ETA 0s42.50 MiB / 44.46 MiB [----------------------------------------------\u0026gt;__] 95.60% 8.03 MiB p/s ETA 0s44.46 MiB / 44.46 MiB [-----------------------------------------------\u0026gt;] 100.00% 8.03 MiB p/s ETA 0s44.46 MiB / 44.46 MiB [-----------------------------------------------\u0026gt;] 100.00% 8.03 MiB p/s ETA 0s44.46 MiB / 44.46 MiB [-----------------------------------------------\u0026gt;] 100.00% 7.72 MiB p/s ETA 0s44.46 MiB / 44.46 MiB [-----------------------------------------------\u0026gt;] 100.00% 7.72 MiB p/s ETA 0s44.46 MiB / 44.46 MiB [-----------------------------------------------\u0026gt;] 100.00% 7.72 MiB p/s ETA 0s44.46 MiB / 44.46 MiB [-----------------------------------------------\u0026gt;] 100.00% 7.22 MiB p/s ETA 0s44.46 MiB / 44.46 MiB [--------------------------------------------------] 100.00% 9.01 MiB p/s 5.1s2024-03-20T13:39:51.416Z\tINFO\tVulnerability scanning is enabled 2024-03-20T13:39:51.416Z\tINFO\tSecret scanning is enabled 2024-03-20T13:39:51.416Z\tINFO\tIf your scanning is slow, please try \u0026#39;--scanners vuln\u0026#39; to disable secret scanning 2024-03-20T13:39:51.416Z\tINFO\tPlease see also https://aquasecurity.github.io/trivy/v0.50/docs/scanner/secret/#recommendation for faster secret detection 2024-03-20T13:39:51.923Z\tINFO\tDetected OS: alpine 2024-03-20T13:39:51.924Z\tINFO\tDetecting Alpine vulnerabilities... 2024-03-20T13:39:51.925Z\tINFO\tNumber of language-specific files: 0 test-nginx-01:latest (alpine 3.18.6) ==================================== Total: 0 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 0) まとめ イメージのスキャンと Dockerfile の診断を行いました。 簡単にセキュリティ診断を行うことができるので、積極的に利用していきたいです。 おまけ docker-compose で実行する場合は以下のようになります。 ubuntu@test-vm-01:~$ cat docker-compose.yml version: \u0026#34;3\u0026#34; services: trivy-scan: image: bitnami/trivy:latest platform: linux/arm64 volumes: - ./test:/workdir working_dir: \u0026#34;/workdir\u0026#34; entrypoint: sh -c \u0026#39;trivy config --ignorefile .trivy/.trivyignore --severity HIGH,CRITICAL .\u0026#39; ","permalink":"https://h-neco.github.io/blog/docker-trivy/","tags":["Security"],"title":"Trivy を用いたコンテナImage,DockerFileへの脆弱性診断"},{"categories":null,"contents":"ChatGPT+Mermaid でシーケンス図を作成する ChatGPT と Mermaid を使って、シーケンス図を作成してみました。\nChatGPT と Mermaid を組み合わせることで、シーケンス図を簡単に作成できます。ChatGPT は自然な文章生成に優れており、Mermaid はシンプルな記法でシーケンス図を描画できます。この組み合わせにより、手軽に要約や説明を行いながら、視覚的な図を生成できます。AI が生成したテキストを元に、Mermaid を使ってシーケンス図を作成することで、複雑なプロセスやアルゴリズムを視覚的に理解しやすくなります。\n簡単なバッチの図を作成してみました プロンプトでの指示 「\u0026ldquo;a\u0026quot;が\u0026quot;b\u0026quot;に対して\u0026quot;x\u0026quot;を非同期で要求し、その後\u0026quot;b\u0026quot;の状態を確認し続けるというプロセスを Mermaid のシーケンス図で作成してください。状態が変更されていない場合は、\u0026ldquo;a\u0026quot;が状態確認を継続し、状態が変更された場合は、\u0026ldquo;b\u0026quot;が\u0026quot;a\u0026quot;にその旨を通知するシーケンスも図に含めてください。」\n回答 あなたが求めているシーケンス図は、以下のような形になります。\nmermaid sequenceDiagram participant a participant b a-\u0026gt;\u0026gt;b: xを非同期で要求 loop 状態確認 a-\u0026gt;\u0026gt;b: bの状態を確認 opt 変更なし a-\u0026gt;\u0026gt;a: 状態の確認を継続 end b--\u0026gt;\u0026gt;a: 状態変更あり end このシーケンス図は、\u0026ldquo;a\u0026quot;が\u0026quot;b\u0026quot;に非同期で\u0026quot;x\u0026quot;を要求し、その後\u0026quot;b\u0026quot;の状態を確認し続けるプロセスを表現しています。\u0026ldquo;b\u0026quot;の状態が変更されない場合、\u0026ldquo;a\u0026quot;は状態の確認を続けます。一方、\u0026ldquo;b\u0026quot;の状態が変更された場合、\u0026ldquo;b\u0026quot;は\u0026quot;a\u0026quot;にその旨を通知します。\n","permalink":"https://h-neco.github.io/blog/chatgpt-mermaid/","tags":["Tools"],"title":"ChatGPT+Mermaidでシーケンス図を作成する"},{"categories":null,"contents":"CloudFrontFunction / LambdaEdge について CloudFront Functions は、Amazon CloudFront のサービスであり、Lambda@Edge より手前で動作し、シンプルな処理を高速かつ安価に実行できるように設計された機能です。従来の Lambda@Edge は、CloudFront のエッジロケーションで動作するカスタムの AWS Lambda 関数を作成して、リクエストまたはレスポンスを処理することができます。一方、CloudFront Functions はよりシンプルで軽量であり、Lambda@Edge よりも低いレイテンシと高いパフォーマンスを持っています。\nCloudFront Functions を使用すると、よりユーザーに近いエッジロケーションでリクエストを処理できるため、レスポンス時間が短縮され、ユーザーエクスペリエンスが向上します。また、コスト面でも Lambda@Edge よりも経済的であるため、シンプルな処理には CloudFront Functions が適しています。\nさらに、CloudFront Functions と Lambda@Edge を組み合わせて使用することも可能です。例えば、シンプルな処理やヘッダーの操作、URL の書き換えなどのタスクを CloudFront Functions で処理し、より高度な処理を必要とする場合には Lambda@Edge で追加の処理を行うことができます。 これにより、より効率的で高速なコンテンツデリバリーサービスを提供することができるため、AWS のクラウドインフラストラクチャを活用したウェブアプリケーションやコンテンツの配信を行う際に便利です。\n比較表 特徴 CloudFrontFunction Lambda@Edge ランタイムサポート JavaScript (ECMAScript 5.1 準拠) Node.js、Python 実行場所 218 以上の CloudFront エッジロケーション 13 の CloudFront リージョンのエッジキャッシュ サポートされる CloudFront トリガー ビューアリクエスト、ビューアレスポンス、オリジンリクエスト、オリジンレスポンス ビューアリクエスト、ビューアレスポンス、オリジンリクエスト、オリジンレスポンス 最大実行時間 1 ミリ秒未満 5 秒 (ビューアトリガー)、30 秒 (オリジントリガー) 最大メモリ 2 MB 128 MB (ビューアトリガー)、10 GB (オリジントリガー) 合計パッケージサイズ 10 KB 1 MB (ビューアトリガー)、50 MB (オリジントリガー) ネットワークアクセス なし あり ファイルシステムアクセス なし あり リクエスト本文へのアクセス なし あり 料金 無料利用枠あり。リクエストごとに課金。 無料利用枠なし。リクエストと関数の実行時間ごとに課金。 まとめ・考察 どちらにするかは処理時間か鍵になりそうです。\nCloudFrontFunctions は同時実行数のクォータとかが公開されておらず、おそらく無いのではないかと思ってます。\nCloudFrontFunctions サンプルコード 実際に使ってみたので CloudFrontFunctions サンプルコードを載っけてみます。\nLambdaEdge はロール作ったりデプロイするのが面倒なのですが、CloudFrontFunctions はコードだけ書けばいいので楽ですね。\nIP 制限をかける function handler(event) { var request = event.request; var viewer = event.viewer; var allowIP = [\u0026#34;1.1.1.1\u0026#34;, \u0026#34;0.0.0.0\u0026#34;]; if (allowIP.indexOf(viewer.ip) !== -1) { return request; } return { statusCode: 403, statusDescription: \u0026#34;Forbidden\u0026#34;, body: \u0026#34;Access to this resource is forbidden.\u0026#34;, }; } www なしから www ありにリダイレクトさせる var querystring = require(\u0026#34;querystring\u0026#34;); // クエリストリングを文字列化するヘルパー関数 function stringifyQueryString(eventQueryString) { var query = {}; Object.entries(eventQueryString).forEach(function (q) { query[q[0]] = q[1].multiValue ? q[1].multiValue.map(function (m) { return m.value; }) : q[1].value; }); return querystring.stringify(query); } function handler(event) { var request = event.request; var newurl = \u0026#34;https://www.domain.jp\u0026#34; + request.uri; // クエリストリングが存在する場合は、文字列化して newurl に追加 if (Object.keys(request.querystring).length \u0026gt; 0) newurl += \u0026#34;?\u0026#34; + stringifyQueryString(request.querystring); return { statusCode: 301, statusDescription: \u0026#34;Found\u0026#34;, headers: { location: { value: newurl } }, }; } Basic 認証をかける（path によって認証を変える） 処理時間を気にして、予め ID,PW を base64 でエンコードしてます。 function handler(event) { var request = event.request; var headers = request.headers; var viewer = event.viewer; var idpw = \u0026#34;dGVzdF8xOnRlc3RfMg==\u0026#34;; // common IDPW var allowPathFlag = false; var optionPathSettingFlag = false; var pathRules = { \u0026#34;^/test/*\u0026#34;: { idpw: \u0026#34;dGVzdF8xOnRlc3RfMw==\u0026#34; }, \u0026#34;^/aiu/*\u0026#34;: { idpw: \u0026#34;dGVzdF8xOnRlc3RfMw==\u0026#34; }, \u0026#34;^/hoge/aiu/*\u0026#34;: { idpw: null }, }; for (var path in pathRules) { if (pathRules.hasOwnProperty(path) \u0026amp;\u0026amp; new RegExp(path).test(request.uri)) { allowPathFlag = true; if (pathRules[path].idpw !== null) { optionPathSettingFlag = true; idpw = pathRules[path].idpw; break; } } } if (allowPathFlag \u0026amp;\u0026amp; !optionPathSettingFlag) { return { request: request }; } // IDPW var authString = \u0026#34;Basic \u0026#34; + idpw; if (typeof headers.authorization === \u0026#34;undefined\u0026#34; || headers.authorization.value !== authString) { return { statusCode: 401, statusDescription: \u0026#34;Unauthorized\u0026#34;, headers: { \u0026#34;www-authenticate\u0026#34;: { value: \u0026#34;Basic\u0026#34; } }, request: request, }; } return { request: request }; } terraform でデプロイするとき 超カンタン。こんな感じでデプロイできます。\nresource \u0026#34;aws_cloudfront_function\u0026#34; \u0026#34;hoge\u0026#34; { name = \u0026#34;hoge-${terraform.workspace}\u0026#34; runtime = \u0026#34;cloudfront-js-1.0\u0026#34; publish = true code = file(\u0026#34;hogehoge.js\u0026#34;) } ","permalink":"https://h-neco.github.io/blog/aws-cloudfront-functions/","tags":["AWS"],"title":"CloudFrontFunctionとLambdaEdgeの違い"},{"categories":null,"contents":"はじめに docker で terraform を実行するための docker-compose.yml を作成しました。\nメリット 環境の統一 Docker は環境の統一を容易にします。異なるプラットフォームや OS で作業している場合でも、Docker コンテナ内で Terraform を実行することで、実行環境を一貫させることができます。 バージョン管理 Terraform のバージョン管理は重要です。異なるプロジェクトや環境で複数の Terraform バージョンを使用する場合、Docker コンテナ内で Terraform を実行することで、各プロジェクトや環境ごとに適切な Terraform バージョンを管理することができます。 windows ユーザーは tfenv が使えないので、docker で管理するのが良いかと思います。 やること 環境変数のセット export AWS_DEFAULT_REGION=ap-northeast-1 export AWS_ACCESS_KEY_ID=xxxxxx export AWS_SECRET_ACCESS_KEY=xxxxxx docker-compose.yml ファイルの作成 version: \u0026#34;3\u0026#34; services: terraform: image: hashicorp/terraform:1.5.2 platform: linux/x86_64 volumes: - ~/.aws:/root/.aws - ./:/workdir working_dir: \u0026#34;/workdir\u0026#34; environment: - AWS_ACCESS_KEY_ID - AWS_SECRET_ACCESS_KEY - AWS_DEFAULT_REGION entrypoint: sh -c \u0026#39;terraform init \u0026amp;\u0026amp; terraform workspace select \u0026#34;${WORKSPACE}\u0026#34; 2\u0026gt;/dev/null || terraform workspace new \u0026#34;${WORKSPACE}\u0026#34; \u0026amp;\u0026amp; terraform \u0026#34;${COMMAND}\u0026#34;\u0026#39; terraform の実行 # コードフォーマッターの実行 export WORKSPACE=\u0026#34;prod\u0026#34; COMMAND=\u0026#34;fmt\u0026#34; \u0026amp;\u0026amp; docker-compose -p プロジェクト名 run --rm terraform # コード構文チェックの実行 export WORKSPACE=\u0026#34;prod\u0026#34; COMMAND=\u0026#34;validate\u0026#34; \u0026amp;\u0026amp; docker-compose -p プロジェクト名 run --rm terraform # 実行計画の確認 export WORKSPACE=\u0026#34;prod\u0026#34; COMMAND=\u0026#34;plan\u0026#34; \u0026amp;\u0026amp; docker-compose -p プロジェクト名 run --rm terraform ","permalink":"https://h-neco.github.io/blog/docker-terraform/","tags":["DevOps"],"title":"dockerでterraformを実行する"},{"categories":null,"contents":"はじめに BitBucketPipelines で image の push を行っていましたが、cpu がマルチプラットフォームに対応しておらず、x86_64 のみしか対応していないため、イメージの push を CodeBuild で行うことにしました。 arm64 で ECS タスクを実行するとコストが 20％落ち、使い方によってはパフォーマンスが向上するらしい。\n今まで [BitBucketPipelines]　→ [awsECR] 変更後 [BitBucketPipelines]　→ [awsCodeBuild]　→ [awsECR] 結果 コストが 20%ぐらい落ちた。パフォーマンスはまだサービスインしてないため分からない\nやること CodeBuild の設定 構成 /image/以下の Dockerfile を build して ECR に push する /image/以下に buildspec.yml を配置 Bitbucket から CodeBuild を実行するための script 用意 bitbuket-pipelines.yml の修正 CodeBuild の設定 data \u0026#34;aws_region\u0026#34; \u0026#34;this\u0026#34; {} resource \u0026#34;aws_codebuild_project\u0026#34; \u0026#34;ecr\u0026#34; { badge_enabled = false build_timeout = 20 // 20分 concurrent_build_limit = 1 // 並列実行数 encryption_key = \u0026#34;arn:aws:kms:${data.aws_region.this.name}:${data.aws_caller_identity.self.account_id}:alias/aws/s3\u0026#34; name = \u0026#34;ecr-build-${terraform.workspace}\u0026#34; project_visibility = \u0026#34;PRIVATE\u0026#34; queued_timeout = 480 service_role = aws_iam_role.codebuild_role.arn tags = { Environment = \u0026#34;${terraform.workspace}\u0026#34; Name = \u0026#34;ecr-build-${terraform.workspace}\u0026#34; } environment { compute_type = \u0026#34;BUILD_GENERAL1_SMALL\u0026#34; // 1vCPU, 3.75GB image = \u0026#34;aws/codebuild/amazonlinux2-aarch64-standard:3.0\u0026#34; type = \u0026#34;ARM_CONTAINER\u0026#34; image_pull_credentials_type = \u0026#34;CODEBUILD\u0026#34; privileged_mode = true // dockerコマンドを叩く場合は必須 } artifacts { type = \u0026#34;NO_ARTIFACTS\u0026#34; } source { location = \u0026#34;bucketname/buildspec/\u0026#34; buildspec = \u0026#34;codebuild/${terraform.workspace}/buildspec.yml\u0026#34; report_build_status = false type = \u0026#34;S3\u0026#34; } } #------------------------------------------------------------# # code build role #------------------------------------------------------------# data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;codebuild_role\u0026#34; { statement { sid = \u0026#34;1\u0026#34; effect = \u0026#34;Allow\u0026#34; actions = [\u0026#34;sts:AssumeRole\u0026#34;] principals { type = \u0026#34;Service\u0026#34; identifiers = [ \u0026#34;codebuild.amazonaws.com\u0026#34; ] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;codebuild_role\u0026#34; { name = \u0026#34;codebuild-ecr-build-${terraform.workspace}\u0026#34; assume_role_policy = data.aws_iam_policy_document.codebuild_role.json } #------------------------------------------------------------# # code build atached policy s3 access #------------------------------------------------------------# resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;codebuild_s3_access_policy\u0026#34; { name = \u0026#34;codebuild-s3-access-policy-${terraform.workspace}\u0026#34; description = \u0026#34;codebuild_s3_access_policy\u0026#34; policy = \u0026lt;\u0026lt;EOT { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::bucketname/buildspec/\u0026#34;, \u0026#34;arn:aws:s3:::bucketname/buildspec/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::bucketname\u0026#34; ], \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ] } ] } EOT } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;codebuild_s3_access_policy\u0026#34; { role = aws_iam_role.codebuild_role.name policy_arn = aws_iam_policy.codebuild_s3_access_policy.arn } #------------------------------------------------------------# # code build atached ecr push policy #------------------------------------------------------------# resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;codebuild_ecr_push_policy\u0026#34; { name = \u0026#34;codebuild-ecr-push-policy-${terraform.workspace}\u0026#34; description = \u0026#34;codebuild_ecr_push_policy\u0026#34; policy = \u0026lt;\u0026lt;EOT { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ecr\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:BatchCheckLayerAvailability\u0026#34;, \u0026#34;ecr:CompleteLayerUpload\u0026#34;, \u0026#34;ecr:GetAuthorizationToken\u0026#34;, \u0026#34;ecr:InitiateLayerUpload\u0026#34;, \u0026#34;ecr:PutImage\u0026#34;, \u0026#34;ecr:UploadLayerPart\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } EOT } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;codebuild_ecr_push_policy\u0026#34; { role = aws_iam_role.codebuild_role.name policy_arn = aws_iam_policy.codebuild_ecr_push_policy.arn } buildspec yal の配置 この辺を参考にして作成して配置 https://docs.aws.amazon.com/ja_jp/codebuild/latest/userguide/sample-docker.html BitBucket から codebuild を実行する用のスクリプト #!/bin/bash env=$1 # codebuildの実行結果を取得する build_output=$(aws codebuild start-build --project-name nnslink-ecr-build-${env}) build_id=$(echo \u0026#34;$build_output\u0026#34; | jq -r \u0026#39;.build.id\u0026#39;) count=0 while : ; do # codebuildの状態を取得する build_status=$(aws codebuild batch-get-builds --ids $build_id --query \u0026#39;builds[0].[buildStatus]\u0026#39; --output text) if [ \u0026#34;$build_status\u0026#34; = \u0026#34;SUCCEEDED\u0026#34; ]; then echo \u0026#34;done!\u0026#34; break elif [ \u0026#34;$build_status\u0026#34; = \u0026#34;FAILED\u0026#34; ]; then exit 1 fi # 10分以上かかったら失敗扱いで終了 count=$((count+1)) if [ $count -gt 20 ]; then echo \u0026#34;[timeout] check aws codebuild console for more detail.\u0026#34; exit 1 fi echo \u0026#34;prosessing...\u0026#34; sleep 30 done exit 0 BitbucketPipeline の設定 definitions: steps: - step: \u0026amp;ECR-push name: ECR push image: atlassian/pipelines-awscli:latest trigger: manual oidc: true script: - export AWS_WEB_IDENTITY_TOKEN_FILE=$(pwd)/web-identity-token - echo $BITBUCKET_STEP_OIDC_TOKEN \u0026gt; $(pwd)/web-identity-token - aws s3 sync ${BITBUCKET_CLONE_DIR}/image/ s3://bucketname/buildspec/ --delete - /bin/bash ./scripts/codebuild.sh ${ENV} pipelines: custom: image-update: - step: \u0026lt;\u0026lt;: *ECR-push deployment: Staging - step: \u0026lt;\u0026lt;: *ECR-push deployment: Production ","permalink":"https://h-neco.github.io/blog/aws-codebuild/","tags":["AWS"],"title":"codebuild構築とdeployフローの整備"},{"categories":null,"contents":"Intro EFS バーストモードのクレジット枯渇対策メモ EFS をバーストモードで使用していて、クレジットが不足してきたため、私が取った対策をメモとして提供します。 技術要素 EFS aws EFS の 2 つのモード EFS には 2 つのモードがあります: バーストモード（汎用）とプロビジョンドモード。\nバーストモードは、使用されているストレージの量に基づいてトラフィックを自動的に調整し、一時的なトラフィックの増加に対応できます。バースト可能な帯域幅はトラフィックの使用状況によって異なり、最低 105 Mbps のバーストが可能です。ただし、1 秒あたりの読み取り/書き込みリクエストの数には制限があり、制限を超えるとスループットが低下する可能性があります。\nプロビジョンドモードでは、ボリュームのスループット、最小/最大/バースト、および書き込みスループットを設定できます。読み取りスループットは書き込みスループットの 3 倍です。プロビジョンドモードはトラフィックに基づいた自動スケーリングはありませんが、スループットの設定に基づいて必要なトラフィックに対応できます。\nバーストモード（汎用）の注意事項と対策 バーストモード（汎用）の注意事項と対策\n注意事項 1: バーストクレジット\nバーストモードは、ファイルの読み取り/書き込み操作から蓄積されたクレジットを消費し、NAS 上のデータ使用状況に基づいてクレジットを補充します。バーストモードの最低速度は 105 Mbps です。ただし、バーストクレジットが枯渇すると、スループットの性能が著しく低下し、マウントシステムからのファイル参照がタイムアウトする場合があります。\n確認方法:\nメトリック名: BurstCreditBalance 最初に 2.3T のクレジットが提供されます。 バーストクレジットの枯渇を避けるためには、以下の対策を取ることができます:\nクレジットの回復を加速するために大量のデータを配置する。 プロビジョンドモードに切り替えて一定のスループットを確保する。 注意事項 2: リクエスト制限\nバーストモードにはリクエストの制限があります。 リード用の最大 IOPS は 35,000、ライト用の最大 IOPS は 7,000 です。 バーストモードの最大 IOPS を超えると、スループットのパフォーマンスが低下します。したがって、高い IOPS が必要な場合は、プロビジョンドモードの使用を検討してください。 確認方法: メトリック名: PercentIOLimit 検討に基づく対策の取得 バーストモードに大量のデータを配置してクレジットの回復速度を加速するという戦略を採用しました。\n10GB のファイルを配置する試み。 コスト: $3/月 コマンド: $ dd if=/dev/zero of=10GB_file_1 bs=10240k count=1000 確認方法: メトリック名: BurstCreditBalance 高いコストのため、プロビジョンドモードは使用していません。 スループット 33(Mib/s) / 最大読み取りスループット 99(Mib/s) コスト: $238/月 スループット 15(Mib/s) / 最大読み取りスループット 45(Mib/s) コスト: $108/月 スループット 5(Mib/s) / 最大読み取りスループット 15(Mib/s) コスト: $36/月 ","permalink":"https://h-neco.github.io/blog/aws-efs/","tags":["AWS"],"title":"EFS Burst Mode のクレジット枯渇対策メモ"},{"categories":null,"contents":"Intro デフォルトでは、EC2 インスタンスが終了すると、アタッチされた EBS ボリュームも削除されます。しかし、AWS CLI を使用してこれらを永続化する方法を探ってみましょう。 技術要素 EC2/EBS コマンド タグによってフィルタリングされた対象インスタンスのボリューム情報を取得します。 DeleteOnTermination が true である場合、ボリュームは永続化されていないことを意味します。 $ aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=xxxxx-prod-web01\u0026#34; | jq -r .Reservations[0].Instances[0].BlockDeviceMappings [ { \u0026#34;DeviceName\u0026#34;: \u0026#34;/dev/sda1\u0026#34;, \u0026#34;Ebs\u0026#34;: { \u0026#34;AttachTime\u0026#34;: \u0026#34;2023-04-18T04:59:14+00:00\u0026#34;, \u0026#34;DeleteOnTermination\u0026#34;: true, \u0026#34;Status\u0026#34;: \u0026#34;attached\u0026#34;, \u0026#34;VolumeId\u0026#34;: \u0026#34;vol-xxxxxxxxxxxx\u0026#34; } } ] 設定ファイルを準備します。 DeleteOnTermination を false に設定します。 $ vim mapping.json [ { \u0026#34;DeviceName\u0026#34;: \u0026#34;/dev/sda1\u0026#34;, \u0026#34;Ebs\u0026#34;: { \u0026#34;DeleteOnTermination\u0026#34;: false } } ] インスタンスの設定変更 $ aws ec2 modify-instance-attribute --instance-id \u0026#34;i-xxxxxxxxxxxxxx\u0026#34; --block-device-mappings file://mapping.json タグでフィルタリングされた対象インスタンスのボリューム情報を取得します。 DeleteOnTermination が false であることを確認します。 $ aws ec2 describe-instances --filters \u0026#34;Name=tag:Name,Values=xxxxx-prod-web01\u0026#34; | jq -r .Reservations[0].Instances[0].BlockDeviceMappings [ { \u0026#34;DeviceName\u0026#34;: \u0026#34;/dev/sda1\u0026#34;, \u0026#34;Ebs\u0026#34;: { \u0026#34;AttachTime\u0026#34;: \u0026#34;2023-04-18T04:59:14+00:00\u0026#34;, \u0026#34;DeleteOnTermination\u0026#34;: false, \u0026#34;Status\u0026#34;: \u0026#34;attached\u0026#34;, \u0026#34;VolumeId\u0026#34;: \u0026#34;vol-xxxxxxxxxxxx\u0026#34; } } ] ","permalink":"https://h-neco.github.io/blog/aws-ec2-ebs-delete-on-termination/","tags":["AWS"],"title":"Persisting an Attached EBS Volume to EC2 Using AWS CLI."},{"categories":null,"contents":"Intro 以前は、GitHub や Bitbucket から個人の AWS へのデプロイ時にアクセスキーとシークレットキーを使用していました。しかし、これを管理するのが煩雑になったため、OIDC に切り替えました。\nOIDC (OpenID Connect) は、認証と認可のためのオープンスタンダードです。OIDC を使用することで、AWS へのアクセスを安全に管理し、認証プロバイダを介してシームレスなアクセス体験を提供できます。OIDC を使用する場合、AWS へのアクセスにはアクセストークンが使用されます。\nOIDC を導入することで、アクセスキーやシークレットキーを個別に管理する必要がなくなり、セキュリティと利便性が向上します。\nやること Terraform で OIDC を設定する方法 GitHub Bitbucket OIDC を使用してデプロイする方法 シンプルな GitHub Actions と Bitbucket Pipelines の作成 技術要素 OIDC AWS Bitbucket GitHub Terraform What is OIDC? OIDC（OpenID Connect）は、OAuth 2.0 プロトコルを拡張し、Web アプリケーションやモバイルアプリケーションにおけるユーザー認証の仕組みを提供する認証プロトコルです。\nBenefits of OIDC Git からのデプロイに OIDC を使用することで、いくつかの利点があります。それには、Git を介したデプロイのセキュリティの向上、トークンの有効期限の管理の容易化、MFA が有効化されていないアクセスキーの使用に関連するセキュリティリスクの回避などがあります。ただし、MFA が有効化されたアクセスキーを使用する場合は、デバイス管理などの要素を考慮する必要があるため、複雑な場合があります。OIDC を使用することで、ユーザーは ID プロバイダーの認証資格情報を使用できるため、認証情報の管理が簡素化され、セキュリティが向上します。\n構築背景 以前は、AWS へのデプロイは手間がかかるものでした。CLI の代わりに管理コンソールからデプロイすることを考えるほどです。\n以前のフローでは、キーの取得に不便さが伴いました： アプリから MFA トークンを確認してコマンドを実行する： $ aws sts get-session-token \u0026ndash;serial-number arn:aws:iam::xxxxxx:mfa/xxxxxx \u0026ndash;token-code xxxxxx 取得したキーをエクスポートするか、Git に登録する セッションの有効期限が切れるたびに定期的にコマンドを実行する\u0026hellip; Terraform で OIDC を構築する Terraform での構築 GitHub Actions を使用するシナリオと Bitbucket Pipelines を使用するシナリオの 2 つを説明します。 GitHub Sample variable \u0026#34;aws_account_id\u0026#34; {} variable \u0026#34;github_repo_name\u0026#34; {} variable \u0026#34;oidc_token_url\u0026#34; { default = \u0026#34;https://token.actions.githubusercontent.com\u0026#34; } data \u0026#34;tls_certificate\u0026#34; \u0026#34;github_oidc_token\u0026#34; { url = var.oidc_token_url } resource \u0026#34;aws_iam_openid_connect_provider\u0026#34; \u0026#34;github_oidc_provider\u0026#34; { url = var.oidc_token_url client_id_list = [ \u0026#34;sts.amazonaws.com\u0026#34; ] thumbprint_list = [data.tls_certificate.github_oidc_token.certificates.0.sha1_fingerprint] } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;github_oidc_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Federated\u0026#34; identifiers = [\u0026#34;arn:aws:iam::${var.aws_account_id}:oidc-provider/oidc-provider/token.actions.githubusercontent.com\u0026#34;] } condition { test = \u0026#34;StringEquals\u0026#34; variable = \u0026#34;token.actions.githubusercontent.com:sub\u0026#34; values = [\u0026#34;repo:${var.github_repo_name}:ref:refs/heads/main\u0026#34;] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;github_oidc_role\u0026#34; { name = \u0026#34;GithubOIDC-TEST\u0026#34; description = \u0026#34;GithubOIDC-TEST\u0026#34; assume_role_policy = data.aws_iam_policy_document.github_oidc_policy.json } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;github_oidc_administrator_access_attachment\u0026#34; { role = aws_iam_role.github_oidc_role.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/AdministratorAccess\u0026#34; } Bitbucket Sample variable \u0026#34;bitbucket_oidc_url\u0026#34; {} variable \u0026#34;bitbucket_oidc_audience\u0026#34; {} variable \u0026#34;account_id\u0026#34; {} variable \u0026#34;git_space\u0026#34; {} data \u0026#34;tls_certificate\u0026#34; \u0026#34;bitbucket\u0026#34; { url = var.bitbucket_oidc_url } resource \u0026#34;aws_iam_openid_connect_provider\u0026#34; \u0026#34;bitbucket\u0026#34; { url = var.bitbucket_oidc_url client_id_list = [ var.bitbucket_oidc_audience, ] thumbprint_list = [data.tls_certificate.bitbucket.certificates.0.sha1_fingerprint] } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;bitbucket_oidc_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;] effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Federated\u0026#34; identifiers = [\u0026#34;arn:aws:iam::${var.account_id}:oidc-provider/api.bitbucket.org/2.0/workspaces/${var.git_space}/pipelines-config/identity/oidc\u0026#34;] } condition { test = \u0026#34;StringEquals\u0026#34; variable = \u0026#34;api.bitbucket.org/2.0/workspaces/${var.git_space}/pipelines-config/identity/oidc:aud\u0026#34; values = [var.bitbucket_oidc_audience] } } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;bitbucket_oidc_role\u0026#34; { name = \u0026#34;BitbucketOIDC-TEST\u0026#34; description = \u0026#34;BitbucketOIDC-TEST\u0026#34; assume_role_policy = data.aws_iam_policy_document.bitbucket_oidc_policy.json } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;administrator_access_attachment\u0026#34; { role = aws_iam_role.bitbucket_oidc_role.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/AdministratorAccess\u0026#34; } Testing OIDC Setup Github Action The sample code deploys statically built Hugo content to S3 using OIDC. name: s3-deploy on: push: branches: - main jobs: s3put: runs-on: ubuntu-latest permissions: id-token: write contents: read steps: - name: Checkout code uses: actions/checkout@v2 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.111.3\u0026#39; - name: Build run: hugo --minify - uses: aws-actions/configure-aws-credentials@v1 with: aws-region: \u0026#39;ap-northeast-1\u0026#39; # Specify the region role-to-assume: \u0026#39;arn:aws:iam::xxxxxxxxxxx:role/oidc-role\u0026#39; # ARN of the created IAM role - name: Deploy run: aws s3 sync --delete public s3://my-s3-bucket/ Bitbucket Pipelines The sample code syncs the \u0026ldquo;public\u0026rdquo; folder from the master branch to my-s3-bucket using OIDC. image: amazon/aws-cli pipelines: default: - step: \u0026amp;s3-deploy name: Deploy to S3 with OIDC oidc: true script: - export AWS_WEB_IDENTITY_TOKEN_FILE=$(pwd)/web-identity-token - export AWS_ROLE_ARN=\u0026#39;created role\u0026#39; - echo $BITBUCKET_STEP_OIDC_TOKEN \u0026gt; $(pwd)/web-identity-token - aws s3 sync --delete public s3://my-s3-bucket/ branches: master: - step: *s3-deploy Setting up OIDC with CloudFormation Parameters: AwsAccountId: Type: String Description: AWS Account ID GithubRepoName: Type: String Description: Name of the GitHub repository OidcTokenUrl: Type: String Default: https://token.actions.githubusercontent.com Resources: GithubOidcTokenCertificate: Type: AWS::CloudFormation::CustomResource Version: \u0026#34;1.0\u0026#34; Properties: ServiceToken: !Sub arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:token-actions-github Url: !Ref OidcTokenUrl GithubOidcProvider: Type: AWS::IAM::OpenIDConnectProvider Properties: Url: !Ref OidcTokenUrl ClientIDList: - sts.amazonaws.com ThumbprintList: - !GetAtt GithubOidcTokenCertificate.Sha1Fingerprint GithubOidcPolicyDocument: Type: AWS::IAM::Policy Properties: PolicyName: GithubOIDCPolicy PolicyDocument: Version: \u0026#34;2012-10-17\u0026#34; Statement: - Effect: Allow Action: sts:AssumeRoleWithWebIdentity Resource: \u0026#34;*\u0026#34; Condition: StringEquals: token.actions.githubusercontent.com:sub: - !Sub \u0026#34;repo:${GithubRepoName}:ref:refs/heads/main\u0026#34; Principal: Federated: !Sub arn:aws:iam::${AwsAccountId}:oidc-provider/oidc-provider/token.actions.githubusercontent.com GithubOidcRole: Type: AWS::IAM::Role Properties: RoleName: GithubOIDC-TEST AssumeRolePolicyDocument: !Ref GithubOidcPolicyDocument GithubOidcAdministratorAccessAttachment: Type: AWS::IAM::PolicyAttachment Properties: PolicyArn: arn:aws:iam::aws:policy/AdministratorAccess Roles: - !Ref GithubOidcRole ","permalink":"https://h-neco.github.io/blog/cicd-oidc/","tags":["DevOps"],"title":"OIDCでのデプロイメモ (Git to AWS)"},{"categories":null,"contents":"Intro PC の入れ替えに伴い、Lambda のローカル実行環境を再構築した。メモとして残しておきます。 PC リプレースのついでに Lambda のローカル実行環境を再構築。メモとして残しておく。\n技術要素 Volta Lambda LocalStack TypeScript SAM 前提条件 Installing Volta Volta のインストール Volta は Node.js のバージョン管理に特化したツールです。プロジェクトごとに異なる Node.js のバージョンを切り替えることができ、Node.js のバージョンを手動でインストール・管理することなく、異なるバージョンを簡単に管理することができます。Volta は、他のバージョン管理ツールと比較して切り替えが簡単という利点があり、OS やシェルに依存しないため様々な環境で利用することができます。\n$ curl https://get.volta.sh | bash $ echo \u0026#39;export VOLTA_HOME=\u0026#34;$HOME/.volta\u0026#34;\u0026#39; \u0026gt;\u0026gt; .zshrc $ echo \u0026#39;export PATH=\u0026#34;$VOLTA_HOME/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; .zshrc Node.js と Yarn のインストール 利用可能なバージョンを確認するには、volta list node を使用します。特定のバージョンをインストールしたい場合は、volta install node@18.15.0 というフォーマットを使ってください。\n$ volta install node success: installed and set node@18.15.0 (with npm@9.5.0) as default $ volta install yarn success: installed and set yarn@4.0.0-rc.42 as default SAM のインストール 以下の手順は macOS 用である。Windows ユーザーの方は公式サイトから MSI ファイルをダウンロードすることをお勧めします。\nmacOS $ brew tap aws/tap $ brew install aws-sam-cli $ sam --version SAM CLI, version 1.78.0 Creating a Project $ sam init -r nodejs18.x SAM CLI now collects telemetry to better understand customer needs. You can OPT OUT and disable telemetry collection by setting the environment variable SAM_CLI_TELEMETRY=0 in your shell. Thanks for your help! Learn More: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-telemetry.html Which template source would you like to use? 1 - AWS Quick Start Templates 2 - Custom Template Location Choice: 1 Choose an AWS Quick Start application template 1 - Hello World Example 2 - Hello World Example With Powertools 3 - Multi-step workflow 4 - Standalone function 5 - Scheduled task 6 - Data processing 7 - Serverless API Template: 1 Based on your selections, the only Package type available is Zip. We will proceed to selecting the Package type as Zip. Based on your selections, the only dependency manager available is npm. We will proceed copying the template using npm. Select your starter template 1 - Hello World Example 2 - Hello World Example TypeScript Template: 2 Would you like to enable X-Ray tracing on the function(s) in your application? [y/N]: n Would you like to enable monitoring using CloudWatch Application Insights? For more info, please view https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-application-insights.html [y/N]: n Project name [sam-app]: sam-local-study Cloning from https://github.com/aws/aws-sam-cli-app-templates (process may take a moment) ----------------------- Generating application: ----------------------- Name: sam-local-study Runtime: nodejs18.x Architectures: x86_64 Dependency Manager: npm Application Template: hello-world-typescript Output Directory: . Configuration file: sam-local-study/samconfig.toml Next steps can be found in the README file at sam-local-study/README.md Commands you can use next ========================= [*] Create pipeline: cd sam-local-study \u0026amp;\u0026amp; sam pipeline init --bootstrap [*] Validate SAM template: cd sam-local-study \u0026amp;\u0026amp; sam validate [*] Test Function in the Cloud: cd sam-local-study \u0026amp;\u0026amp; sam sync --stack-name {stack-name} --watch プロンプトに従うと、以下のファイルが生成される：\n$ tree . . └── sam-local-study ├── README.md ├── events │ └── event.json ├── hello-world │ ├── app.ts │ ├── jest.config.ts │ ├── package.json │ ├── tests │ │ └── unit │ │ └── test-handler.test.ts │ └── tsconfig.json ├── samconfig.toml └── template.yaml 6 directories, 9 files メッセージを返すサンプル Lambda 関数の作成 メッセージを返す Lambda 関数のサンプルを作成し、SAM を使って実行してみましょう。\nコードの準備 $ tree . . └── sam-local-study ├── src │ ├── handlers │ └── hello-from-lambda.js └── template.yaml src/handlers/hello-from-lambda.js exports.helloFromLambdaHandler = async () =\u0026gt; { const message = \u0026#34;Hello from Lambda!\u0026#34;; console.info(`${message}`); return message; }; template.yaml Resources: HelloWorldFunction: Type: AWS::Serverless::Function Properties: Handler: src/handlers/hello-from-lambda.helloFromLambdaHandler Runtime: nodejs18.x MemorySize: 128 Timeout: 100 Description: A Lambda function that returns a static string. Policies: - AWSLambdaBasicExecutionRole \u0026ldquo;Hello \u0026ldquo;を返すラムダ関数の実行 Execute the command: $ sam local invoke HelloWorldFunction Output: Invoking src/handlers/hello-from-lambda.helloFromLambdaHandler (nodejs18.x) Local image is up-to-date Using local image: public.ecr.aws/lambda/nodejs:18-rapid-x86_64. Mounting /lambda-sam-localstack/sam-local-study as /var/task:ro,delegated, inside runtime container START RequestId: 6d2615de-b38f-4300-bf87-704e1fa6296a Version: $LATEST 2023-04-05T08:41:27.339Z\t6d2615de-b38f-4300-bf87-704e1fa6296a\tINFO\tHello from Lambda! END RequestId: 6d2615de-b38f-4300-bf87-704e1fa6296a REPORT RequestId: 6d2615de-b38f-4300-bf87-704e1fa6296a\tInit Duration: 0.75 ms\tDuration: 953.12 ms\tBilled Duration: 954 ms\tMemory Size: 128 MB\tMax Memory Used: 128 MB \u0026#34;Hello from Lambda!\u0026#34;% LocalStack を使う クレデンシャルの設定 (~/.aws/credentials) [localstack] aws_access_key_id = dummy aws_secret_access_key = dummy docker-compose.yml を用意する docker-compose.yml を使って LocalStack を起動する コマンド：docker-compose up -d version: \u0026#39;3.7\u0026#39; services: localstack: image: localstack/localstack ports: - \u0026#39;4566:4566\u0026#39; environment: - SERVICES=s3,dynamodb - PERSISTENCE=1 volumes: - \u0026#39;${LOCALSTACK_VOLUME_DIR:-./volume}:/var/lib/localstack\u0026#39; networks: - localstack networks: localstack: name: sam-local-localstack SAM を使って LocalStack で実行する sam local invoke HelloWorldFunction \\ -e events/event.json \\ --docker-network sam-local-localstack \\ --profile localstack ローカル環境をプログラムから参照する方法 s3 を作成する aws s3 mb s3://xxxxxxxx --endpoint-url http://localhost:4566 --profile localstack ローカル環境をプログラムから参照する方法。一部抜粋 const endpoint = env.isLocal ? \u0026#34;http://localstack:4566\u0026#34; : undefined; const s3Client = new S3Client({ apiVersion: \u0026#34;2006-03-01\u0026#34;, endpoint: endpoint, forcePathStyle: env.isLocal ? true : false, }); ","permalink":"https://h-neco.github.io/blog/aws-lambda-local-execution/","tags":["AWS"],"title":"Lambdaローカル開発環境構築メモ (SAM | LocalStack | TypeScript)"},{"categories":null,"contents":"Intro Terraform の Wrapper ツール、Terragrunt 導入の検証を行いました。 導入の検証 通常、私は Terraform を使用していますが、次のような不便さがありました： 各デプロイの影響範囲を最小限に抑えるために、Git リポジトリをより小さなものに分割する必要がありました。Terraform のバージョンアップが面倒でした。 コードからリソースの依存関係を理解するのが難しかったです。 depends_on が明示的に使用できない場所でデプロイ順序に注意する必要がありました。 導入後の課題 導入に関して、以下の 2 点について検証が保留されています。もし詳しい方がいたら相談に乗っていただけると幸いです。 Terraform の管理だけでなく、Terragrunt のバージョン管理も行う必要があります。バージョンアップのプロセスはどのように変わるのか もし Terragrunt の開発が停止した場合、スムーズに Terraform のコードに戻せるか 技術要素 terraform terragrunt aws Terragrunt とは Terragrunt は、Terraform を使用してインフラストラクチャを管理するための作業を簡素化するための Terraform Wrapper として知られるオープンソースのツールです。Terragrunt は Terraform が提供する機能を拡張し、モジュールのコード再利用、柔軟な構成、再利用性の向上を可能にします。特に、Terraform を使用して複数の環境やアカウントを管理する際に Terragrunt は非常に便利です。\nインストール $ brew install tfenv # Installs Terraform (version specified in .terraform-version) $ brew install tgenv # Installs Terragrunt (version specified in .terragrunt-version) ###　フォルダ構成\nこのフォルダ構成は、他の記事を参考に作成されました。この構造の利点は、各環境ごとに異なる変数を指定できるため、コードの再利用性が向上することです。\n$ tree . . ├── README.md ├── docs │ └── graph-dependencies.png ├── envs │ ├── prod │ │ ├── ResourceGroupA │ │ │ └── terragrunt.hcl │ │ ├── ResourceGroupB │ │ │ └── terragrunt.hcl │ │ └── env.hcl │ └── terragrunt.hcl └── modules ├── ResourceGroupA │ └── xx.tf └── ResourceGroupB └── xx.tf 共通ファイル（envs/terragrunt.hcl） このファイルには、バックエンドとして AWS S3 バケットに*.tfstate ファイルを保存するための設定が含まれており、各環境に独自の*.tfstate ファイルを S3 バケット内に保存することができます。また、AWS リソースを作成するために Terraform が使用するプロバイダの定義も含まれています。設定では、各 AWS リージョンごとに異なるプロバイダ設定が指定されています。\n*.tfstate ファイルを使用してリソースの状態を管理することで、複数の人がプロジェクトに取り組んでいても競合を回避することができます。\n# Configuration for storing *.tfstate files for each environment remote_state { backend = \u0026#34;s3\u0026#34; config = { bucket = \u0026#34;tfstate-xxxxxxxxxxxxxx\u0026#34; # Stored in `stg/modA.tfstate` key = \u0026#34;${path_relative_to_include()}.tfstate\u0026#34; region = \u0026#34;ap-northeast-1\u0026#34; encrypt = true } generate = { path = \u0026#34;backend.tf\u0026#34; if_exists = \u0026#34;overwrite\u0026#34; } } generate \u0026#34;provider\u0026#34; { path = \u0026#34;provider.tf\u0026#34; if_exists = \u0026#34;overwrite_terragrunt\u0026#34; contents = \u0026lt;\u0026lt;EOF terraform { required_version = \u0026#34;\u0026gt;= 1.3.7\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.53.0\u0026#34; } } } # Tokyo region provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-1\u0026#34; alias = \u0026#34;tokyo\u0026#34; } # Virginia region provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; alias = \u0026#34;virginia\u0026#34; } # ... Other regions EOF } 環境固有のファイル（envs/prod/env.hcl） このファイルは、Terragrunt の環境固有のファイルであり、prod 環境の設定が含まれています。これらの変数は、他の Terragrunt ファイルで使用され、環境固有の設定を定義するために使用されます。具体的には、envs/prod/ResourceGroupA の hcl ファイルで呼び出され、Terraform コードの module/ResourceGroupA にパラメータとして渡されます。\nlocals { ENV = \u0026#34;prod\u0026#34; PROJECT_NAME = \u0026#34;test\u0026#34; ACCOUNT_ID = \u0026#34;123456789\u0026#34; VPC_CIDE = \u0026#34;10.0.0.0/24\u0026#34; } 各リソースグループのための HCL ファイル 「ResourceGroupA」と「ResourceGroupB」の HCL ファイルの例を示しましょう。\n最初は依存関係がないと仮定し、A というクリーンなスレートのシナリオを考えます。B は A に依存するリソースグループとします。\n「ResourceGroupA」の HCL ファイルの作成 vim envs/prod/ResourceGroupA/terragrunt.hcl env.hcl から変数を受け取り、それらをモジュールに値として渡します。 locals { ENV = read_terragrunt_config(find_in_parent_folders(\u0026#34;env.hcl\u0026#34;)) } # Include definition of all environments (envs/terragrunt.hcl) include { path = find_in_parent_folders() } terraform { # Reference the module source = \u0026#34;../../../modules//ResourceGroupA\u0026#34; } # Specify the input values for the module inputs = { ENV = local.ENV.locals.ENV PROJECT_NAME = local.ENV.locals.PROJECT_NAME ACCOUNT_ID = local.ENV.locals.ACCOUNT_ID } 「ResourceGroupB」のファイルの作成： vim envs/prod/ResourceGroupB/terragrunt.hcl env.hcl から変数を取得し、それらをモジュールに渡します。 この例では、ResourceGroupA で作成した VPC の ID を ResourceGroupB に渡すことを想定しています。 locals { ENV = read_terragrunt_config(find_in_parent_folders(\u0026#34;env.hcl\u0026#34;)) } # Include definition of all environments (envs/terragrunt.hcl) include { path = find_in_parent_folders() } terraform { # Reference the module source = \u0026#34;../../../modules//ResourceGroupB\u0026#34; } # Specify the input values for the module inputs = { ENV = local.ENV.locals.ENV PROJECT_NAME = local.ENV.locals.PROJECT_NAME ACCOUNT_ID = local.ENV.locals.ACCOUNT_ID VPC_ID = local.ENV.locals.VPC_ID } モジュールの tf ファイル 一般的には、通常どおり Terraform のコードを記述できます。 他のモジュールからパラメータを受け取りたい場合（依存関係がある場合や env.hcl からパラメータを渡す場合）、変数ブロックで空の変数を定義する必要があります。上記で言及した ResourceGroupB の例では、次のようになります。 variable \u0026#34;ENV\u0026#34; { description = \u0026#34;Environment\u0026#34; type = string } variable \u0026#34;PROJECT_NAME\u0026#34; { description = \u0026#34;Project Name\u0026#34; type = string } variable \u0026#34;vpc_id\u0026#34; { description = \u0026#34;The ID of the VPC\u0026#34; type = string } 1 つのリソースグループから別のリソースグループにパラメータを渡す場合は、それらを出力として定義する必要があります。例えば、Resource Group A の場合、以下のように定義する必要があります： output \u0026#34;vpc_id\u0026#34; { value = aws_vpc.main.id } Deployment Terragrunt Commands Formatting cd envs/prod terragrunt run-all hclfmt terragrunt run-all fmt validate cd envs/prod terragrunt validate-all plan cd envs/prod terragrunt run-all plan apply cd envs/prod terragrunt run-all apply Dependency Graph tips Installing dot command on M1 Mac: Graphviz is a graph visualization tool that includes the dot command. You can install Graphviz by running the following command in the terminal: brew install graphviz Verify if the dot command is installed: dot -V cd envs/terragrunt-prod terragrunt graph-dependencies | dot -Tpng \u0026gt; graph-dependencies.png Deployment from git GithubAction name: Terragrunt Actions on: push: branches: - master env: TERRAGRUNT_CACHE_DIR: ${{ github.workspace }}/tool jobs: build: runs-on: ubuntu-latest env: TARGET_ENV: \u0026#39;\u0026#39; steps: - name: Set Production run: | mkdir -p ${GITHUB_WORKSPACE}/deploy echo \u0026#39;export TARGET_ENV=\u0026#34;prod\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${GITHUB_WORKSPACE}/deploy/.env if: github.ref == \u0026#39;refs/heads/master\u0026#39; env: GITHUB_WORKSPACE: ${{ github.workspace }} - name: Set Staging run: | mkdir -p ${GITHUB_WORKSPACE}/deploy echo \u0026#39;export TARGET_ENV=\u0026#34;stg\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${GITHUB_WORKSPACE}/deploy/.env if: github.ref == \u0026#39;refs/heads/staging\u0026#39; env: GITHUB_WORKSPACE: ${{ github.workspace }} - name: Terragrunt Plan env: TERRAGRUNT_DOWNLOAD: \u0026#34;https://github.com/gruntwork-io/terragrunt/releases/download/v0.34.0/terragrunt_linux_amd64\u0026#34; run: | source ${GITHUB_WORKSPACE}/deploy/.env sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y curl unzip git curl -Lo /tmp/terragrunt.zip ${TERRAGRUNT_DOWNLOAD} sudo unzip -d /usr/local/bin /tmp/terragrunt.zip cd ${GITHUB_WORKSPACE}/envs/${TARGET_ENV} terragrunt run-all plan if: github.ref == \u0026#39;refs/heads/master\u0026#39; || github.ref == \u0026#39;refs/heads/staging\u0026#39; env: GITHUB_WORKSPACE: ${{ github.workspace }} TERRAGRUNT_CACHE_DIR: ${{ env.TERRAGRUNT_CACHE_DIR }} - name: Terragrunt Apply if: github.ref == \u0026#39;refs/heads/master\u0026#39; env: TERRAGRUNT_DOWNLOAD: \u0026#34;https://github.com/gruntwork-io/terragrunt/releases/download/v0.34.0/terragrunt_linux_amd64\u0026#34; run: | source ${GITHUB_WORKSPACE}/deploy/.env sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y curl unzip git curl -Lo /tmp/terragrunt.zip ${TERRAGRUNT_DOWNLOAD} sudo unzip -d /usr/local/bin /tmp/terragrunt.zip cd ${GITHUB_WORKSPACE}/envs/${TARGET_ENV} terragrunt run-all apply --terragrunt-non-interactive env: GITHUB_WORKSPACE: ${{ github.workspace }} TERRAGRUNT_CACHE_DIR: ${{ env.TERRAGRUNT_CACHE_DIR }} Bitbucket-Pipelines image: hashicorp/terraform:1.3.7 definitions: caches: terragrunt: ${BITBUCKET_CLONE_DIR}/tool steps: - step: \u0026amp;set-production name: Set Production script: - mkdir -p ${BITBUCKET_CLONE_DIR}/deploy - echo \u0026#39;export TARGET_ENV=\u0026#34;prod\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${BITBUCKET_CLONE_DIR}/deploy/.env artifacts: - deploy/** - step: \u0026amp;set-staging name: Set Staging script: - mkdir -p ${BITBUCKET_CLONE_DIR}/deploy - echo \u0026#39;export TARGET_ENV=\u0026#34;stg\u0026#34;\u0026#39; \u0026gt;\u0026gt; ${BITBUCKET_CLONE_DIR}/deploy/.env artifacts: - deploy/** - step: \u0026amp;terragrunt-plan name: terragrunt Plan caches: - terragrunt script: - source ${BITBUCKET_CLONE_DIR}/deploy/.env - apk update \u0026amp;\u0026amp; apk add bash curl groff jq less unzip git - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tfenv ]; then git clone https://github.com/tfutils/tfenv.git ${BITBUCKET_CLONE_DIR}/tool/tfenv; fi - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tgenv ]; then git clone https://github.com/cunymatthieu/tgenv.git ${BITBUCKET_CLONE_DIR}/tool/tgenv; fi - export PATH=${BITBUCKET_CLONE_DIR}/tool/tfenv/bin:$PATH - export PATH=${BITBUCKET_CLONE_DIR}/tool/tgenv/bin:$PATH - cd ${BITBUCKET_CLONE_DIR}/envs/${TARGET_ENV} - tfenv install \u0026amp;\u0026amp; tgenv install - terragrunt run-all plan - step: \u0026amp;terragrunt-apply name: terragrunt Apply trigger: manual caches: - terragrunt script: - source ${BITBUCKET_CLONE_DIR}/deploy/.env - apk update \u0026amp;\u0026amp; apk add bash curl groff jq less unzip git - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tfenv ]; then git clone https://github.com/tfutils/tfenv.git ${BITBUCKET_CLONE_DIR}/tool/tfenv; fi - if [ ! -d ${BITBUCKET_CLONE_DIR}/tool/tgenv ]; then git clone https://github.com/cunymatthieu/tgenv.git ${BITBUCKET_CLONE_DIR}/tool/tgenv; fi - export PATH=${BITBUCKET_CLONE_DIR}/tool/tfenv/bin:$PATH - export PATH=${BITBUCKET_CLONE_DIR}/tool/tgenv/bin:$PATH - cd ${BITBUCKET_CLONE_DIR}/envs/${TARGET_ENV} - tfenv install \u0026amp;\u0026amp; tgenv install - terragrunt run-all apply --terragrunt-non-interactive pipelines: default: - step: *set-production - step: *terragrunt-plan branches: master: - step: *set-production - step: *terragrunt-plan - step: \u0026lt;\u0026lt;: *terragrunt-apply deployment: production ","permalink":"https://h-neco.github.io/blog/cicd-terragrunt-1/","tags":["DevOps"],"title":"TerraformのWrapperツール、Terragrunt導入の検証を行いました。"},{"categories":null,"contents":"Intro AmazonLinux2 を使用して NAT インスタンスを設定しました。構成プロセスに Packer と Ansible を使用しました。\nAmazonLinux2 を使用して NAT インスタンスを設定します。 Amazonlinux2 が選択する理由 Lambda は外部から開始される着信接続をサポートしていないため、アクティブモードで FTP をサポートしません。 ECS は、プライベート IP アドレスの静的割り当てを許可していません。 Gateway タイプであるため、NAT プライベートゲートウェイと組み合わせることで静的割り当てを実現することは可能ですが、外部ソースからの着信接続を受け入れることはできません。 技術要素 Packer Ansible iptables CENTOS7（AmazonLinux2）では、デフォルトのファイアウォール管理システムはファイアウォールです。ただし、IPTables を紹介して、NAT インスタンスをセットアップします。 ファイル構造 $ tree . . ├── ansible.cfg ├── bin │ └── init.sh ├── inventory │ └── hosts ├── packer-template │ └── nat_instance.json ├── playbook │ └── setup.yml └── roles └── iptable └── tasks ├── main.yml └── templates ├── iptables-config.j2 ├── nat_cidr.j2 └── sysctl.conf Packer とテンプレートファイルの実行 Execution Command packer build packer-template/nat_instance.json . └── packer-template └── nat_instance.json { \u0026#34;variables\u0026#34;: { \u0026#34;aws_access_key\u0026#34;: \u0026#34;{{env `AWS_ACCESS_KEY`}}\u0026#34;, \u0026#34;aws_secret_key\u0026#34;: \u0026#34;{{env `AWS_SECRET_KEY`}}\u0026#34; }, \u0026#34;builders\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;amazon-ebs\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;{{user `aws_access_key`}}\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;{{user `aws_secret_key`}}\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-northeast-1\u0026#34;, \u0026#34;ami_regions\u0026#34;: [ \u0026#34;ap-northeast-1\u0026#34; ], \u0026#34;associate_public_ip_address\u0026#34;: true, \u0026#34;source_ami\u0026#34;: \u0026#34;ami-0a3d21ec6281df8cb\u0026#34;, \u0026#34;instance_type\u0026#34;: \u0026#34;t3.micro\u0026#34;, \u0026#34;ssh_username\u0026#34;: \u0026#34;ec2-user\u0026#34;, \u0026#34;ami_name\u0026#34;: \u0026#34;nat-{{timestamp}}\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;nat-instance\u0026#34; }, \u0026#34;ena_support\u0026#34;: true, \u0026#34;enable_t2_unlimited\u0026#34;: false } ], \u0026#34;provisioners\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;shell\u0026#34;, \u0026#34;script\u0026#34;: \u0026#34;./bin/init.sh\u0026#34;, \u0026#34;pause_before\u0026#34;: \u0026#34;60s\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;ansible-local\u0026#34;, \u0026#34;inventory_file\u0026#34;: \u0026#34;inventory/hosts\u0026#34;, \u0026#34;playbook_file\u0026#34;: \u0026#34;playbook/setup.yml\u0026#34;, \u0026#34;role_paths\u0026#34;: [ \u0026#34;roles/iptable\u0026#34; ], \u0026#34;staging_directory\u0026#34;: \u0026#34;/tmp/ansible-local\u0026#34;, \u0026#34;extra_arguments\u0026#34;: [\u0026#34;-vv\u0026#34;] }] } Ansible の準備 ファイル構成 . ├── ansible.cfg ├── bin │ └── init.sh └── inventory └── hosts Install ansible Ansible インストールは、 。/bin/init.shを実行することで実行できます\n#!/bin/bash sudo yum -y update # ansible install amazon-linux-extras install epel amazon-linux-extras enable ansible2 amazon-linux-extras install ansible2 インベントリファイルの構成 inventory/hosts ファイルを配置します。\n設定したサーバーからローカルで Ansible を実行するため、ターゲットとして「ローカル」を指定します。\n[default] 127.0.0.1 構成ファイルを配置します ansible.cfgファイルを配置します\nこのファイルは、Ansible の実行に使用されます。以下はサンプル構成です。\n[defaults] # Specify the Python interpreter interpreter_python=/usr/bin/python3 # Location of the hosts file inventory = inventroy/hosts # Whether to validate host keys when connecting host_key_checking = True # Number of parallel processes to use for remote connections forks=5 # Path to the log file log_path=~/logs/ansible/ansible.log # Color settings nocolor=0 scp_if_ssh=True Ansible Playbook を作成します file Structure . ├── playbook │ └── setup.yml └── roles └── iptable └── tasks ├── main.yml └── templates ├── iptables-config.j2 ├── nat_cidr.j2 └── sysctl.conf Packer によって呼び出される Playbook/setup.yml ファイルを作成します。 iptables のロールを呼び出し、ルーティング後に出口時にソースアドレス/ポート変換を実行するチェーンを渡します。\n--- - hosts: all become: yes become_user: root remote_user: ec2-user roles: - role: iptable vars: nat_cidr: 10.0.0.1/24 また、playbook から呼び出される iptables ロールの main.yml ファイルも書きます。主な構成は、iptables のインストールと必要な設定ファイルの構成です。\nroles/iptable/tasks/main.yml --- - name: Install iptables-service yum: name=iptables-services state=latest - name: Enable port forwarding template: src=\u0026#34;sysctl.conf\u0026#34; dest=\u0026#34;/etc/sysctl.conf\u0026#34; owner=root group=root mode=0644 # IP tables - name: Make iptables file template: src=\u0026#34;{{ item }}.j2\u0026#34; dest=\u0026#34;/etc/sysconfig/{{ item }}\u0026#34; owner=root group=root mode=0600 with_items: - \u0026#34;nat_cidr\u0026#34; # IP tables config - name: Make iptables-config file template: src=\u0026#34;iptables-config.j2\u0026#34; dest=\u0026#34;/etc/sysconfig/iptables-config-nat\u0026#34; owner=root group=root mode=0600 - name: Install ftp command yum: name=ftp state=latest - name: Install tcpdump command yum: name=tcpdump state=latest - name: Install lftp command yum: name=lftp state=latest 上記で使用したテンプレート・ファイルを書き込んでいきます。 roles/iptable/tasks/templates/iptables-config.j2 roles/iptable/tasks/templates/sysctl.conf roles/iptable/tasks/templates/nat_cidr.j2 roles/iptable/tasks/templates/iptables-config.j2\nIPTABLES_MODULES=\u0026#34;nf_conntrack_ftp nf_nat_ftp\u0026#34; IPTABLES_MODULES_UNLOAD=\u0026#34;yes\u0026#34; IPTABLES_SAVE_ON_STOP=\u0026#34;no\u0026#34; IPTABLES_SAVE_ON_RESTART=\u0026#34;no\u0026#34; IPTABLES_SAVE_COUNTER=\u0026#34;no\u0026#34; IPTABLES_STATUS_NUMERIC=\u0026#34;yes\u0026#34; IPTABLES_STATUS_VERBOSE=\u0026#34;no\u0026#34; IPTABLES_STATUS_LINENUMBERS=\u0026#34;yes\u0026#34; roles/iptable/tasks/templates/sysctl.conf\nIPv4 フォワーディングと呼ばれる機能を有効にするには、「/proc/sys/net/ipv4/ip_forward」ファイルの内容を「1」に設定する。\nnet.ipv4.ip_forward=1 roles/iptable/tasks/templates/nat_cidr.j2\n*filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] COMMIT *nat :POSTROUTING ACCEPT [0:0] -A POSTROUTING -s {{ nat_cidr }} -j MASQUERADE COMMIT *raw :PREROUTING ACCEPT [0:0] -A PREROUTING -p tcp --dport 21 -j CT --helper ftp COMMIT EC2 のネットワーク設定を構成するには インスタンス構成で、NAT インスタンスの Elastic Network Interface (ENI)の宛先チェックを無効にします。\naws ec2 modify-instance-attribute \\ --no-source-dest-check \\ --instance-id ${INSTANCE_ID} サーバーに接続して iptables を起動するには Ansible を使って設定ファイルをデプロイし、iptables を再起動するには\n# 設定ファイルをAnsibleで配布されているファイルで上書きする sudo cp /etc/sysconfig/iptables-config-nat /etc/sysconfig/iptables # 設定の永続化 sudo service iptables save # サービスを再起動する sudo systemctl restart iptables ","permalink":"https://h-neco.github.io/blog/aws-ec2-nat-instance/","tags":["AWS"],"title":"PackerとAnsibleを使用してAmazon Linux 2にNATインスタンスを作成します。"},{"categories":null,"contents":"背景 NFS から s3 へデータを移行して WEB ホスティングするように基盤移行を行った際、404 のオブジェクトが多数発生していた。 また、s3 のオブジェクト名・プレフィックス名には禁則文字というものが存在するうらしい。 何が対象なのか判断するために、ブラウザのコンソールログを取得する必要があった。\n動作環境 環境・ツール\n$ sw_vers ProductName: Mac OS X ProductVersion: 10.15.2 BuildVersion: 19C57 $ python3 -V Python 3.7.3z $ pip3 list | grep -e chrome -e selenium chromedriver-binary 83.0.4103.39.0 selenium 3.141.0 install 手順\n$ sudo pip3 install selenium $ sudo pip3 install chromedriver-binary==83.0.4103.39.0 技術要素 selenium web driver python ec2 ファイル構成 ./log/ (ログ出力先ディレクトリ) ./config.ini (設定ファイル) ./grep.py (実行ファイル) ./url.txt (検索対象のURLリスト) 実行準備 クローリング対象の URL を記載する $ vi url.txt http://user:password@google.com/ http://www.google.com/ ... config ファイルの配置 $ vi config.ini [INTERVAL] TIME_SEC = 5 [URL] FILE = url.txt [RESPONSE_CODE] OK = 200 [LOG] DIR = ./log/ TARGET = browser LEVEL = SEVERE TEXT_START = START TEXT_DONE = DONE 実行ファイルの配置 #!/usr/bin/env python3 # -*- coding: utf-8 -*- # selenium from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities from selenium.webdriver.chrome.options import Options import chromedriver_binary # other modules import re import os import signal import datetime import logging import time import configparser # iniファイルの読み込み conf = configparser.ConfigParser() conf.read(\u0026#39;config.ini\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) # ログ設定 logging.basicConfig( filename= conf[\u0026#39;LOG\u0026#39;].get(\u0026#39;DIR\u0026#39;) + datetime.datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) + \u0026#39;.log\u0026#39;, level=logging.INFO, format=\u0026#39;%(asctime)s %(levelname)s %(message)s\u0026#39; ) # chrome webdriver ログレベルの指定 ※オプション設定の影響を受ける caps = DesiredCapabilities.CHROME caps[\u0026#34;goog:loggingPrefs\u0026#34;] = {conf[\u0026#39;LOG\u0026#39;].get(\u0026#39;TARGET\u0026#39;):conf[\u0026#39;LOG\u0026#39;].get(\u0026#39;LEVEL\u0026#39;)} # chrome オプション設定 options = Options() options.add_argument(\u0026#39;--headless\u0026#39;) # ブラウザーのUIなしにする options.add_argument(\u0026#39;--ignore-certificate-errors\u0026#39;) # SSL証明書失効サイト対策 # 他にも適宜オプション追加できる。mobileUIにするなど。 def main(): try: logging.info(conf[\u0026#39;LOG\u0026#39;].get(\u0026#39;TEXT_START\u0026#39;)) # Webdriver browser = webdriver.Chrome(desired_capabilities=caps,options=options) f = open(conf[\u0026#39;URL\u0026#39;].get(\u0026#39;FILE\u0026#39;)) urlList = f.read() f.close() urls = urlList.split(\u0026#39;\\n\u0026#39;) count = 1 for url in urls: if len(url) == 0: logging.info(conf[\u0026#39;LOG\u0026#39;].get(\u0026#39;TEXT_DONE\u0026#39;)) break else: # URLのステータスチェック status = requests.get(str(url)) # 対象URL及びステータスのログ出力 target_url_log = str(count) + \u0026#34;:\u0026#34; + str(status) + \u0026#34;:\u0026#34; + str(url) print(target_url_log) # ステータスコードが200の場合はINFO、それ以外はWARNINGでログ出力 logging.info(target_url_log) if status.status_code == int(conf[\u0026#39;RESPONSE_CODE\u0026#39;].get(\u0026#39;OK\u0026#39;)) else logging.warning(target_url_log) if status.status_code == int(conf[\u0026#39;RESPONSE_CODE\u0026#39;].get(\u0026#39;OK\u0026#39;)): # 対象URLをブラウズし、指定したログを取得する browser.get(url) for _log in browser.get_log(conf[\u0026#39;LOG\u0026#39;].get(\u0026#39;TARGET\u0026#39;)): print(_log) logging.warning(_log) count += 1 # 負荷対策のためsleepをいれる time.sleep(int(conf[\u0026#39;INTERVAL\u0026#39;].get(\u0026#39;TIME_SEC\u0026#39;))) except Exception as e: print(e) logging.error(e) finally: # chromedriver のプロセスが残ってしまうためkillする os.kill(browser.service.process.pid,signal.SIGTERM) if __name__ == \u0026#34;__main__\u0026#34;: main() 実行 $ python3 grep.py 2020-07-13 10:36:59,209 INFO START 2020-07-13 10:37:00,883 INFO 1:\u0026lt;Response [200]\u0026gt;:https://google.com/ 2020-07-13 10:37:10,228 WARNING 2:\u0026lt;Response [404]\u0026gt;:https://google.com/aaa 2020-07-13 10:37:17,021 INFO 3:\u0026lt;Response [200]\u0026gt;:http://www.google.com/googlegooglegooglegooglegoogle... 2020-07-13 10:37:22,576 WARNING {\u0026#39;level\u0026#39;: \u0026#39;SEVERE\u0026#39;, \u0026#39;message\u0026#39;: \u0026#39;xxx\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;network\u0026#39;, \u0026#39;timestamp\u0026#39;: 1594604239554} 2020-07-13 10:37:22,576 WARNING {\u0026#39;level\u0026#39;: \u0026#39;SEVERE\u0026#39;, \u0026#39;message\u0026#39;: \u0026#39;xxx\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;console-api\u0026#39;, \u0026#39;timestamp\u0026#39;: 1594604239823} 2020-07-13 10:37:22,576 WARNING {\u0026#39;level\u0026#39;: \u0026#39;SEVERE\u0026#39;, \u0026#39;message\u0026#39;: \u0026#39;xxx\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;console-api\u0026#39;, \u0026#39;timestamp\u0026#39;: 1594604241138} tips Chrome のバージョン（stable)と、python の chromedriver-binary のバージョンがうまく合わないと、うまく動いてくれません。\nEC2(amazonlinux2)に chrome を入れる方法\n## chrome install (yumは依存で詰まるからcurl) $ curl https://intoli.com/install-google-chrome.sh | bash ## GConf2 リポジトリ追加 (ChromeDeiver動かすのに必要らしい) $ sudo vim /etc/yum.repos.d/centos.repo [CentOS-base] name=CentOS-6 - Base mirrorlist=http://mirrorlist.centos.org/?release=6\u0026amp;arch=x86_64\u0026amp;repo=os gpgcheck=1 gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6 #released updates [CentOS-updates] name=CentOS-6 - Updates mirrorlist=http://mirrorlist.centos.org/?release=6\u0026amp;arch=x86_64\u0026amp;repo=updates gpgcheck=1 gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6 #additional packages that may be useful [CentOS-extras] name=CentOS-6 - Extras mirrorlist=http://mirrorlist.centos.org/?release=6\u0026amp;arch=x86_64\u0026amp;repo=extras gpgcheck=1 gpgkey=http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6\u0026#34; GPG key インポート rpm --import http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-6 ## Google Noto Fontsいれる（キャプチャ取るわけじゃないから入れなくてもいいのかも） $ Gconf2 install yum -y install GConf2 $ mkdir ~dl \u0026amp;\u0026amp; cd dl $ unzip Noto-hinted.zip $ mkdir -p /usr/share/fonts/opentype/noto $ cp *otf *ttf /usr/share/fonts/opentype/noto $ fc-cache -f -v ### python3系いれる $ sudo yum install python3 $ sudo pip3 update $ sudo pip3 install --upgrade pip ### 利用しているchromeのバージョンと合わせる # sudo pip3 install chromedriver-binary==84.0.4147.30 ","permalink":"https://h-neco.github.io/blog/browser-selenium/","tags":["Tools"],"title":"ブラウザのコンソールログをSelenium(ヘッドレスブラウザ)で取得する"},{"categories":null,"contents":"tags tags Recent Posts Trivy を用いたコンテナ Image,DockerFile への脆弱性診断 2024-03-21 multipass で ローカル docker 環境の構築 2024-03-20 chatgpt+mermaid でシーケンス図を作成する 2024-02-21 CloudFrontFunction と LambdaEdge の違い 2023-07-21 Docker から Terraform を実行する 2023-07-10 Codebuild 構築と Deploy フローの整備 2023-06-27 EFS バーストモードクレジット枯渇対策メモ 2023-05-03 AWS CLI を使った EC2 への EBS ボリュームの永続化 2023-05-01 Terraform のラッパーツール、Terragrunt の導入検証を行ってみた 2023-04-27 Lambda ローカル開発環境構築メモ (SAM | LocalStack | TypeScript) 2023-04-24 OIDC (Git to AWS)のデプロイメモ 2023-04-26 Packer と Ansible を使って Amazon Linux 2 に NAT インスタンスを作成する 2023-04-11 ブラウザのコンソールログを Selenium(ヘッドレスブラウザ)で取得する 2020-08-26 ","permalink":"https://h-neco.github.io/blog/list/","tags":[],"title":"List"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"https://h-neco.github.io/search/","tags":null,"title":"Search Results"}]